{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import scipy.stats as stats\n",
    "from importlib import reload\n",
    "from sklearn.utils.extmath import safe_sparse_dot\n",
    "from sklearn.utils import check_array\n",
    "sys.path.insert(0,'/home/users/jvogel/Science/scripts/')\n",
    "import kfold_learning as kfl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'kfold_learning' from '/Users/jakevogel/git/hack_projects/kfold_learning.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(kfl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xdf = pandas.read_csv('/Users/jakevogel/Science/TADPOLE/ISVD_75_cleaned.csv')\n",
    "ydf = pandas.read_csv('/Users/jakevogel/Science/TADPOLE/ISVD_75_cleaned_ref.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tps = {}\n",
    "for rid in ydf.RID.unique():\n",
    "    if type(ydf[ydf.RID==rid]['RID']) == str:\n",
    "        tps.update({rid: 1})\n",
    "    else:\n",
    "        tps.update({rid: len(ydf[ydf.RID==rid]['RID'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(tps.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1667"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.array(list(tps.values()))[np.array(list(tps.values()))>1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1667"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(held_idz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['DX_bl', 'y_DX', 'y_ADAS11', 'y_ADAS13', 'ventr_ICV_ratio'], dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.columns[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for col in ydf.columns[-5:]:\n",
    "#    xdf[col] = ydf[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = xdf.loc[held_idx]\n",
    "tdf = ydf.loc[test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>RID</th>\n",
       "      <th>PTID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>SITE</th>\n",
       "      <th>D1</th>\n",
       "      <th>D2</th>\n",
       "      <th>COLPROT</th>\n",
       "      <th>ORIGPROT</th>\n",
       "      <th>EXAMDATE</th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>y_DX</th>\n",
       "      <th>y_ADAS11</th>\n",
       "      <th>y_ADAS13</th>\n",
       "      <th>ventr_ICV_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>011_S_0002</td>\n",
       "      <td>bl</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>ADNI1</td>\n",
       "      <td>2005-09-08</td>\n",
       "      <td>CN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.08395</td>\n",
       "      <td>0.098839</td>\n",
       "      <td>1.771697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  RID        PTID VISCODE  SITE  D1  D2 COLPROT ORIGPROT  \\\n",
       "0           0    2  011_S_0002      bl    11   1   1   ADNI1    ADNI1   \n",
       "\n",
       "     EXAMDATE DX_bl  y_DX  y_ADAS11  y_ADAS13  ventr_ICV_ratio  \n",
       "0  2005-09-08    CN   1.0  -0.08395  0.098839         1.771697  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tdf.RID.unique()[0]\n",
    "#tdf[tdf.RID==x].index[i+ii]\n",
    "tdf[tdf.RID==x]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18433181140532068"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ydf.loc[ydf[ydf.RID==x].index[i+ii]]['y_ADAS13']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "ii = 2\n",
    "t_y = pandas.Series([ydf.loc[ydf[ydf.RID==x\n",
    "                                        ].index[i+ii]]['y_ADAS13'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "test = pandas.DataFrame(xdf.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.212544\n",
       "1       1.494935\n",
       "2       0.383530\n",
       "3      -0.357846\n",
       "4       0.697288\n",
       "5       2.036104\n",
       "6      -1.069846\n",
       "7       1.523147\n",
       "8      -0.870648\n",
       "9      -0.813368\n",
       "10     -0.443185\n",
       "11     -1.098059\n",
       "12     -0.927073\n",
       "13     -0.357692\n",
       "14     -0.813368\n",
       "15      1.608640\n",
       "16     -0.813368\n",
       "17      0.697288\n",
       "18     -0.785155\n",
       "19     -0.471397\n",
       "20     -0.727875\n",
       "21      0.810993\n",
       "22     -0.214919\n",
       "23     -0.687340\n",
       "24      1.238457\n",
       "25      0.839206\n",
       "26     -1.012566\n",
       "27      1.238457\n",
       "28     -0.214919\n",
       "29      1.010191\n",
       "          ...   \n",
       "1629    0.041559\n",
       "1630    1.152964\n",
       "1631   -0.727875\n",
       "1632   -1.411817\n",
       "1633    0.981979\n",
       "1634   -0.727875\n",
       "1635   -1.240831\n",
       "1636   -0.642383\n",
       "1637   -0.385904\n",
       "1638   -1.326324\n",
       "1639   -1.155339\n",
       "1640   -0.813368\n",
       "1641    2.264369\n",
       "1642   -1.411817\n",
       "1643   -0.707665\n",
       "1644   -0.471397\n",
       "1645   -1.155339\n",
       "1646    1.323949\n",
       "1647   -1.155339\n",
       "1648   -0.471397\n",
       "1649   -0.471397\n",
       "1652   -0.129426\n",
       "1654   -1.069846\n",
       "1656   -0.727875\n",
       "1659   -1.069846\n",
       "1660   -1.240831\n",
       "1661    0.298037\n",
       "1662    0.640008\n",
       "1663   -0.300412\n",
       "1665    0.041559\n",
       "Length: 1590, dtype: float64"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test.shape\n",
    "t_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all(test.index == t_y.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>PTETHCAT</th>\n",
       "      <th>PTRACCAT</th>\n",
       "      <th>PTMARRY</th>\n",
       "      <th>APOE4</th>\n",
       "      <th>CDRSB</th>\n",
       "      <th>MMSE</th>\n",
       "      <th>...</th>\n",
       "      <th>ST97TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST98CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST98SA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST98TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST99CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST99SA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST99TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "      <th>ST9SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.069575</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>1.439562</td>\n",
       "      <td>-0.547293</td>\n",
       "      <td>-0.209333</td>\n",
       "      <td>-0.836143</td>\n",
       "      <td>-0.023183</td>\n",
       "      <td>3.176360</td>\n",
       "      <td>2.627168</td>\n",
       "      <td>1.421115</td>\n",
       "      <td>-0.606512</td>\n",
       "      <td>3.287650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.073477</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.828688</td>\n",
       "      <td>-1.669659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118874</td>\n",
       "      <td>-0.763491</td>\n",
       "      <td>0.269184</td>\n",
       "      <td>-1.464361</td>\n",
       "      <td>-0.492644</td>\n",
       "      <td>0.169014</td>\n",
       "      <td>0.420666</td>\n",
       "      <td>-0.533728</td>\n",
       "      <td>-0.132373</td>\n",
       "      <td>1.716909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.905643</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-2.116339</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.416885</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164266</td>\n",
       "      <td>-0.231419</td>\n",
       "      <td>0.243549</td>\n",
       "      <td>-0.356837</td>\n",
       "      <td>-1.328287</td>\n",
       "      <td>0.605692</td>\n",
       "      <td>1.071538</td>\n",
       "      <td>-0.027816</td>\n",
       "      <td>-0.837174</td>\n",
       "      <td>-1.111001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>-0.016473</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.440952</td>\n",
       "      <td>1.340932</td>\n",
       "      <td>0.841696</td>\n",
       "      <td>1.332369</td>\n",
       "      <td>-0.370584</td>\n",
       "      <td>0.845894</td>\n",
       "      <td>0.584538</td>\n",
       "      <td>0.947581</td>\n",
       "      <td>-0.465551</td>\n",
       "      <td>-0.701054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15</td>\n",
       "      <td>0.944404</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.056117</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>-0.403279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486343</td>\n",
       "      <td>-1.610034</td>\n",
       "      <td>-1.837147</td>\n",
       "      <td>-0.538322</td>\n",
       "      <td>-0.445698</td>\n",
       "      <td>-0.254604</td>\n",
       "      <td>-0.814145</td>\n",
       "      <td>0.903061</td>\n",
       "      <td>0.393025</td>\n",
       "      <td>-1.546838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20</td>\n",
       "      <td>0.227331</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-2.116339</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>1.366533</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>1.362505</td>\n",
       "      <td>-1.669659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542527</td>\n",
       "      <td>-0.717163</td>\n",
       "      <td>-0.200788</td>\n",
       "      <td>-0.575550</td>\n",
       "      <td>-0.342417</td>\n",
       "      <td>-0.579416</td>\n",
       "      <td>-0.389462</td>\n",
       "      <td>-0.614673</td>\n",
       "      <td>-0.580883</td>\n",
       "      <td>-0.826196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>22</td>\n",
       "      <td>1.532403</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875397</td>\n",
       "      <td>-1.108847</td>\n",
       "      <td>-1.106553</td>\n",
       "      <td>0.080588</td>\n",
       "      <td>-0.680429</td>\n",
       "      <td>0.215578</td>\n",
       "      <td>0.921514</td>\n",
       "      <td>-0.307079</td>\n",
       "      <td>-0.529624</td>\n",
       "      <td>-0.017811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24</td>\n",
       "      <td>0.012210</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>1.006627</td>\n",
       "      <td>-0.656555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875397</td>\n",
       "      <td>-1.497723</td>\n",
       "      <td>-1.239000</td>\n",
       "      <td>-0.380104</td>\n",
       "      <td>-2.023090</td>\n",
       "      <td>0.067936</td>\n",
       "      <td>0.540685</td>\n",
       "      <td>-0.222086</td>\n",
       "      <td>-1.093465</td>\n",
       "      <td>-1.531016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>28</td>\n",
       "      <td>0.671916</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213995</td>\n",
       "      <td>-1.671805</td>\n",
       "      <td>-1.606433</td>\n",
       "      <td>-0.519708</td>\n",
       "      <td>0.671621</td>\n",
       "      <td>0.229206</td>\n",
       "      <td>-0.202510</td>\n",
       "      <td>0.700697</td>\n",
       "      <td>0.226435</td>\n",
       "      <td>-1.171414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33</td>\n",
       "      <td>1.001770</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315570</td>\n",
       "      <td>-0.340922</td>\n",
       "      <td>-0.269148</td>\n",
       "      <td>-0.505748</td>\n",
       "      <td>0.173991</td>\n",
       "      <td>-0.054720</td>\n",
       "      <td>0.538377</td>\n",
       "      <td>-0.647052</td>\n",
       "      <td>-0.196445</td>\n",
       "      <td>-0.795989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>36</td>\n",
       "      <td>-1.206814</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-2.469746</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>2.986463</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.359571</td>\n",
       "      <td>0.157457</td>\n",
       "      <td>-0.046979</td>\n",
       "      <td>1.211379</td>\n",
       "      <td>0.793681</td>\n",
       "      <td>-0.669136</td>\n",
       "      <td>-0.696434</td>\n",
       "      <td>0.235258</td>\n",
       "      <td>-1.016578</td>\n",
       "      <td>-0.161652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>41</td>\n",
       "      <td>-0.102522</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.117484</td>\n",
       "      <td>-0.799992</td>\n",
       "      <td>-1.029649</td>\n",
       "      <td>0.248113</td>\n",
       "      <td>-0.652262</td>\n",
       "      <td>-0.803717</td>\n",
       "      <td>-0.885694</td>\n",
       "      <td>-0.027816</td>\n",
       "      <td>-1.131909</td>\n",
       "      <td>-0.760029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>45</td>\n",
       "      <td>-0.174229</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>2.986463</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.845136</td>\n",
       "      <td>0.436830</td>\n",
       "      <td>0.328999</td>\n",
       "      <td>0.815835</td>\n",
       "      <td>-0.708597</td>\n",
       "      <td>-0.503323</td>\n",
       "      <td>-1.019562</td>\n",
       "      <td>1.408973</td>\n",
       "      <td>-1.285684</td>\n",
       "      <td>-0.888047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>49</td>\n",
       "      <td>-1.522326</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.357513</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>2.986463</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>2.528954</td>\n",
       "      <td>-1.365758</td>\n",
       "      <td>-1.858509</td>\n",
       "      <td>1.132270</td>\n",
       "      <td>-1.131113</td>\n",
       "      <td>-0.574305</td>\n",
       "      <td>-1.054182</td>\n",
       "      <td>0.368819</td>\n",
       "      <td>-0.247704</td>\n",
       "      <td>-1.292240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>53</td>\n",
       "      <td>-0.303302</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-0.702709</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>2.986463</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>-0.150003</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.874007</td>\n",
       "      <td>-0.013817</td>\n",
       "      <td>0.427266</td>\n",
       "      <td>-0.100897</td>\n",
       "      <td>-1.778970</td>\n",
       "      <td>0.480765</td>\n",
       "      <td>0.556841</td>\n",
       "      <td>0.239305</td>\n",
       "      <td>0.008588</td>\n",
       "      <td>-0.163090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>60</td>\n",
       "      <td>-1.393253</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.650749</td>\n",
       "      <td>-1.416383</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.329310</td>\n",
       "      <td>0.106917</td>\n",
       "      <td>0.333271</td>\n",
       "      <td>-0.072976</td>\n",
       "      <td>-0.689818</td>\n",
       "      <td>0.598310</td>\n",
       "      <td>1.895514</td>\n",
       "      <td>-0.800849</td>\n",
       "      <td>-0.709028</td>\n",
       "      <td>1.738485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>64</td>\n",
       "      <td>0.557185</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017300</td>\n",
       "      <td>-0.555716</td>\n",
       "      <td>-0.756210</td>\n",
       "      <td>0.117816</td>\n",
       "      <td>0.934520</td>\n",
       "      <td>0.099736</td>\n",
       "      <td>0.146007</td>\n",
       "      <td>-0.108762</td>\n",
       "      <td>0.636502</td>\n",
       "      <td>0.081439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>69</td>\n",
       "      <td>1.360306</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>0.116932</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.319909</td>\n",
       "      <td>-0.505177</td>\n",
       "      <td>-1.140733</td>\n",
       "      <td>0.680885</td>\n",
       "      <td>0.699789</td>\n",
       "      <td>-0.000206</td>\n",
       "      <td>0.302955</td>\n",
       "      <td>-0.173518</td>\n",
       "      <td>-0.286147</td>\n",
       "      <td>0.084316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>71</td>\n",
       "      <td>0.442453</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708962</td>\n",
       "      <td>-1.385412</td>\n",
       "      <td>-1.166368</td>\n",
       "      <td>-0.314956</td>\n",
       "      <td>-1.666299</td>\n",
       "      <td>1.083824</td>\n",
       "      <td>1.161552</td>\n",
       "      <td>0.372866</td>\n",
       "      <td>-0.568068</td>\n",
       "      <td>-1.344023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>75</td>\n",
       "      <td>0.428112</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.416885</td>\n",
       "      <td>-0.403279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>-0.079799</td>\n",
       "      <td>-0.170881</td>\n",
       "      <td>0.164350</td>\n",
       "      <td>-0.436309</td>\n",
       "      <td>0.354134</td>\n",
       "      <td>0.572997</td>\n",
       "      <td>-0.019721</td>\n",
       "      <td>-0.683399</td>\n",
       "      <td>-1.420258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>78</td>\n",
       "      <td>-0.088181</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.651388</td>\n",
       "      <td>0.083051</td>\n",
       "      <td>0.111103</td>\n",
       "      <td>0.206232</td>\n",
       "      <td>-0.173410</td>\n",
       "      <td>-0.623140</td>\n",
       "      <td>-0.280984</td>\n",
       "      <td>-0.234228</td>\n",
       "      <td>-1.260055</td>\n",
       "      <td>-1.095178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>83</td>\n",
       "      <td>-0.418034</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-0.702709</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>2.230409</td>\n",
       "      <td>-0.061007</td>\n",
       "      <td>-0.403279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179396</td>\n",
       "      <td>0.153246</td>\n",
       "      <td>-0.179426</td>\n",
       "      <td>0.918211</td>\n",
       "      <td>-0.624094</td>\n",
       "      <td>0.533007</td>\n",
       "      <td>0.425282</td>\n",
       "      <td>0.271684</td>\n",
       "      <td>-0.606512</td>\n",
       "      <td>0.533099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>84</td>\n",
       "      <td>-0.145546</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.350169</td>\n",
       "      <td>-0.704528</td>\n",
       "      <td>0.713522</td>\n",
       "      <td>-2.041390</td>\n",
       "      <td>-1.609964</td>\n",
       "      <td>-0.887191</td>\n",
       "      <td>-0.200202</td>\n",
       "      <td>-1.007261</td>\n",
       "      <td>-0.298962</td>\n",
       "      <td>-0.761468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>93</td>\n",
       "      <td>0.342063</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.845136</td>\n",
       "      <td>0.556161</td>\n",
       "      <td>0.393086</td>\n",
       "      <td>0.727419</td>\n",
       "      <td>0.117656</td>\n",
       "      <td>0.131536</td>\n",
       "      <td>-0.387154</td>\n",
       "      <td>1.105426</td>\n",
       "      <td>-0.555253</td>\n",
       "      <td>-0.915377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>98</td>\n",
       "      <td>1.718842</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.061007</td>\n",
       "      <td>0.058874</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.041832</td>\n",
       "      <td>-1.216946</td>\n",
       "      <td>-1.251817</td>\n",
       "      <td>-0.142778</td>\n",
       "      <td>-0.502034</td>\n",
       "      <td>-1.280713</td>\n",
       "      <td>-1.104960</td>\n",
       "      <td>-0.954646</td>\n",
       "      <td>-0.004227</td>\n",
       "      <td>-1.185798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>99</td>\n",
       "      <td>1.733184</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>3.315344</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.416885</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.541137</td>\n",
       "      <td>0.987153</td>\n",
       "      <td>0.769064</td>\n",
       "      <td>0.387717</td>\n",
       "      <td>0.493225</td>\n",
       "      <td>-0.193844</td>\n",
       "      <td>0.249869</td>\n",
       "      <td>-0.651099</td>\n",
       "      <td>-0.401479</td>\n",
       "      <td>0.771875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>101</td>\n",
       "      <td>1.561086</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.391223</td>\n",
       "      <td>-0.868783</td>\n",
       "      <td>-0.363142</td>\n",
       "      <td>-0.468520</td>\n",
       "      <td>-0.520812</td>\n",
       "      <td>0.673267</td>\n",
       "      <td>0.669936</td>\n",
       "      <td>0.243353</td>\n",
       "      <td>1.059382</td>\n",
       "      <td>2.620229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>105</td>\n",
       "      <td>0.542843</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>1.006627</td>\n",
       "      <td>-1.669659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.542527</td>\n",
       "      <td>-1.895023</td>\n",
       "      <td>0.153827</td>\n",
       "      <td>-2.688221</td>\n",
       "      <td>-1.685078</td>\n",
       "      <td>-1.823580</td>\n",
       "      <td>2.052462</td>\n",
       "      <td>-4.378655</td>\n",
       "      <td>-1.887969</td>\n",
       "      <td>1.072502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>106</td>\n",
       "      <td>-1.049058</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>2.230409</td>\n",
       "      <td>-0.416885</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.329310</td>\n",
       "      <td>-0.239842</td>\n",
       "      <td>0.141010</td>\n",
       "      <td>-0.631391</td>\n",
       "      <td>-0.210967</td>\n",
       "      <td>0.048061</td>\n",
       "      <td>0.328343</td>\n",
       "      <td>0.113839</td>\n",
       "      <td>-1.285684</td>\n",
       "      <td>0.222403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>112</td>\n",
       "      <td>0.901380</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>0.650749</td>\n",
       "      <td>-0.909831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.194527</td>\n",
       "      <td>-0.321268</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.201578</td>\n",
       "      <td>-0.633483</td>\n",
       "      <td>-0.288675</td>\n",
       "      <td>-0.380230</td>\n",
       "      <td>-0.060194</td>\n",
       "      <td>0.726204</td>\n",
       "      <td>1.229289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1637</th>\n",
       "      <td>10662</td>\n",
       "      <td>0.413770</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>-0.150003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.076431</td>\n",
       "      <td>0.446658</td>\n",
       "      <td>0.175190</td>\n",
       "      <td>0.224845</td>\n",
       "      <td>-0.699208</td>\n",
       "      <td>1.122438</td>\n",
       "      <td>1.064613</td>\n",
       "      <td>0.372866</td>\n",
       "      <td>-1.055022</td>\n",
       "      <td>-0.991613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1638</th>\n",
       "      <td>10667</td>\n",
       "      <td>-0.762229</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.879735</td>\n",
       "      <td>1.263719</td>\n",
       "      <td>0.593893</td>\n",
       "      <td>1.192765</td>\n",
       "      <td>0.399333</td>\n",
       "      <td>0.860658</td>\n",
       "      <td>0.886893</td>\n",
       "      <td>0.231211</td>\n",
       "      <td>-0.388664</td>\n",
       "      <td>0.314461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1639</th>\n",
       "      <td>10670</td>\n",
       "      <td>0.356404</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>1.064327</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.743561</td>\n",
       "      <td>1.624517</td>\n",
       "      <td>1.033958</td>\n",
       "      <td>0.601776</td>\n",
       "      <td>0.803070</td>\n",
       "      <td>0.513132</td>\n",
       "      <td>0.076765</td>\n",
       "      <td>-0.007580</td>\n",
       "      <td>0.713389</td>\n",
       "      <td>0.691324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1640</th>\n",
       "      <td>10678</td>\n",
       "      <td>0.456795</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244256</td>\n",
       "      <td>1.858966</td>\n",
       "      <td>-0.435774</td>\n",
       "      <td>2.798209</td>\n",
       "      <td>2.981373</td>\n",
       "      <td>0.839647</td>\n",
       "      <td>-0.112495</td>\n",
       "      <td>0.870683</td>\n",
       "      <td>1.020939</td>\n",
       "      <td>0.894140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1641</th>\n",
       "      <td>10696</td>\n",
       "      <td>-0.647497</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>1.184566</td>\n",
       "      <td>-0.656555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.894866</td>\n",
       "      <td>0.587046</td>\n",
       "      <td>1.247581</td>\n",
       "      <td>-0.594164</td>\n",
       "      <td>0.718567</td>\n",
       "      <td>-0.408492</td>\n",
       "      <td>-1.169585</td>\n",
       "      <td>0.704744</td>\n",
       "      <td>0.226435</td>\n",
       "      <td>-1.371352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1642</th>\n",
       "      <td>10702</td>\n",
       "      <td>-1.163790</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.015909</td>\n",
       "      <td>0.682510</td>\n",
       "      <td>0.273457</td>\n",
       "      <td>0.210885</td>\n",
       "      <td>-0.361195</td>\n",
       "      <td>1.741397</td>\n",
       "      <td>0.302955</td>\n",
       "      <td>1.384689</td>\n",
       "      <td>1.776998</td>\n",
       "      <td>-0.748522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1643</th>\n",
       "      <td>10705</td>\n",
       "      <td>0.442453</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.304778</td>\n",
       "      <td>1.919333</td>\n",
       "      <td>0.615255</td>\n",
       "      <td>1.979200</td>\n",
       "      <td>1.826497</td>\n",
       "      <td>0.517107</td>\n",
       "      <td>-0.574107</td>\n",
       "      <td>1.558723</td>\n",
       "      <td>0.380210</td>\n",
       "      <td>0.344668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1644</th>\n",
       "      <td>10709</td>\n",
       "      <td>0.815331</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515214</td>\n",
       "      <td>-0.916515</td>\n",
       "      <td>-0.226423</td>\n",
       "      <td>-1.096737</td>\n",
       "      <td>-0.905771</td>\n",
       "      <td>-0.129109</td>\n",
       "      <td>-0.733363</td>\n",
       "      <td>0.429528</td>\n",
       "      <td>-0.017042</td>\n",
       "      <td>0.674063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1645</th>\n",
       "      <td>10715</td>\n",
       "      <td>-0.561449</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.032430</td>\n",
       "      <td>-0.126128</td>\n",
       "      <td>0.542623</td>\n",
       "      <td>-1.157232</td>\n",
       "      <td>-0.830657</td>\n",
       "      <td>0.296781</td>\n",
       "      <td>0.725329</td>\n",
       "      <td>-0.493255</td>\n",
       "      <td>-0.939691</td>\n",
       "      <td>-0.377413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1646</th>\n",
       "      <td>10718</td>\n",
       "      <td>-1.780472</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>1.362505</td>\n",
       "      <td>-0.656555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.985648</td>\n",
       "      <td>0.867823</td>\n",
       "      <td>0.158100</td>\n",
       "      <td>0.629696</td>\n",
       "      <td>0.887573</td>\n",
       "      <td>-0.042795</td>\n",
       "      <td>-0.647965</td>\n",
       "      <td>0.304062</td>\n",
       "      <td>1.149084</td>\n",
       "      <td>-0.683794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1647</th>\n",
       "      <td>10722</td>\n",
       "      <td>-1.235497</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>3.315344</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.058353</td>\n",
       "      <td>1.051732</td>\n",
       "      <td>-0.175153</td>\n",
       "      <td>1.620884</td>\n",
       "      <td>0.774903</td>\n",
       "      <td>0.310409</td>\n",
       "      <td>-0.280984</td>\n",
       "      <td>0.575231</td>\n",
       "      <td>0.303323</td>\n",
       "      <td>0.021026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1648</th>\n",
       "      <td>10728</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.710920</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213995</td>\n",
       "      <td>0.429811</td>\n",
       "      <td>-0.525496</td>\n",
       "      <td>0.299301</td>\n",
       "      <td>1.141083</td>\n",
       "      <td>0.057147</td>\n",
       "      <td>-1.391159</td>\n",
       "      <td>1.485871</td>\n",
       "      <td>1.033753</td>\n",
       "      <td>0.491386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1649</th>\n",
       "      <td>10743</td>\n",
       "      <td>0.327721</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-0.349302</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>0.780795</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.683039</td>\n",
       "      <td>0.717607</td>\n",
       "      <td>0.418721</td>\n",
       "      <td>0.052667</td>\n",
       "      <td>0.859406</td>\n",
       "      <td>1.270079</td>\n",
       "      <td>0.441438</td>\n",
       "      <td>0.449765</td>\n",
       "      <td>2.340839</td>\n",
       "      <td>0.284255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1650</th>\n",
       "      <td>10746</td>\n",
       "      <td>0.628892</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.168604</td>\n",
       "      <td>1.430781</td>\n",
       "      <td>0.482808</td>\n",
       "      <td>1.136924</td>\n",
       "      <td>-0.257914</td>\n",
       "      <td>1.207616</td>\n",
       "      <td>0.566073</td>\n",
       "      <td>0.777595</td>\n",
       "      <td>1.225972</td>\n",
       "      <td>2.146993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1651</th>\n",
       "      <td>10750</td>\n",
       "      <td>1.245574</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.497136</td>\n",
       "      <td>0.800437</td>\n",
       "      <td>-0.145246</td>\n",
       "      <td>1.825636</td>\n",
       "      <td>-0.013793</td>\n",
       "      <td>0.071911</td>\n",
       "      <td>-0.964168</td>\n",
       "      <td>0.563089</td>\n",
       "      <td>2.071733</td>\n",
       "      <td>-0.767221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1652</th>\n",
       "      <td>10753</td>\n",
       "      <td>-0.905643</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>2.048069</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.875397</td>\n",
       "      <td>-0.047510</td>\n",
       "      <td>0.512715</td>\n",
       "      <td>-0.468520</td>\n",
       "      <td>0.990855</td>\n",
       "      <td>0.321198</td>\n",
       "      <td>-0.050178</td>\n",
       "      <td>1.101379</td>\n",
       "      <td>-0.286147</td>\n",
       "      <td>-0.293985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1653</th>\n",
       "      <td>10755</td>\n",
       "      <td>0.600209</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>1.006627</td>\n",
       "      <td>-0.403279</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.587918</td>\n",
       "      <td>0.222036</td>\n",
       "      <td>-0.811752</td>\n",
       "      <td>0.383063</td>\n",
       "      <td>1.037801</td>\n",
       "      <td>-0.805989</td>\n",
       "      <td>-0.673353</td>\n",
       "      <td>-0.800849</td>\n",
       "      <td>-0.068300</td>\n",
       "      <td>1.049488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1654</th>\n",
       "      <td>10758</td>\n",
       "      <td>0.571526</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-0.349302</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.103273</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179396</td>\n",
       "      <td>-0.046106</td>\n",
       "      <td>-0.303328</td>\n",
       "      <td>0.704152</td>\n",
       "      <td>0.183381</td>\n",
       "      <td>0.915172</td>\n",
       "      <td>0.406817</td>\n",
       "      <td>0.554994</td>\n",
       "      <td>1.200343</td>\n",
       "      <td>-0.232134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1655</th>\n",
       "      <td>10761</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.061007</td>\n",
       "      <td>-0.150003</td>\n",
       "      <td>...</td>\n",
       "      <td>1.424431</td>\n",
       "      <td>-1.469645</td>\n",
       "      <td>-1.896961</td>\n",
       "      <td>0.103855</td>\n",
       "      <td>-0.173410</td>\n",
       "      <td>-1.645842</td>\n",
       "      <td>-1.850462</td>\n",
       "      <td>-1.173200</td>\n",
       "      <td>0.700574</td>\n",
       "      <td>0.430972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1656</th>\n",
       "      <td>10766</td>\n",
       "      <td>0.930062</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.064327</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.724092</td>\n",
       "      <td>1.436396</td>\n",
       "      <td>-0.358870</td>\n",
       "      <td>1.848903</td>\n",
       "      <td>2.117563</td>\n",
       "      <td>0.225799</td>\n",
       "      <td>-0.426391</td>\n",
       "      <td>0.134076</td>\n",
       "      <td>1.828257</td>\n",
       "      <td>-0.994490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>10775</td>\n",
       "      <td>-1.106424</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.116932</td>\n",
       "      <td>-0.150003</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.724092</td>\n",
       "      <td>-1.392432</td>\n",
       "      <td>-0.628036</td>\n",
       "      <td>-1.659806</td>\n",
       "      <td>-1.215616</td>\n",
       "      <td>-0.414170</td>\n",
       "      <td>-0.114803</td>\n",
       "      <td>-0.808943</td>\n",
       "      <td>0.405839</td>\n",
       "      <td>0.311585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1658</th>\n",
       "      <td>10776</td>\n",
       "      <td>-2.483203</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-0.702709</td>\n",
       "      <td>4.591811</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>2.074260</td>\n",
       "      <td>-0.403279</td>\n",
       "      <td>...</td>\n",
       "      <td>0.410691</td>\n",
       "      <td>0.328731</td>\n",
       "      <td>1.452660</td>\n",
       "      <td>-1.036242</td>\n",
       "      <td>0.136434</td>\n",
       "      <td>-0.231890</td>\n",
       "      <td>0.582230</td>\n",
       "      <td>-0.974882</td>\n",
       "      <td>-1.541975</td>\n",
       "      <td>-1.886302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1659</th>\n",
       "      <td>10780</td>\n",
       "      <td>0.585868</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.077821</td>\n",
       "      <td>-0.865975</td>\n",
       "      <td>-0.717758</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>-0.079518</td>\n",
       "      <td>-1.177932</td>\n",
       "      <td>-1.979713</td>\n",
       "      <td>0.660224</td>\n",
       "      <td>0.585243</td>\n",
       "      <td>1.117093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1660</th>\n",
       "      <td>10785</td>\n",
       "      <td>-1.135107</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.356549</td>\n",
       "      <td>...</td>\n",
       "      <td>0.909996</td>\n",
       "      <td>1.770521</td>\n",
       "      <td>1.405663</td>\n",
       "      <td>0.834449</td>\n",
       "      <td>-0.924549</td>\n",
       "      <td>1.195691</td>\n",
       "      <td>0.349116</td>\n",
       "      <td>1.089237</td>\n",
       "      <td>0.508356</td>\n",
       "      <td>-0.380290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1661</th>\n",
       "      <td>10791</td>\n",
       "      <td>2.335525</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>1.417734</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.863100</td>\n",
       "      <td>...</td>\n",
       "      <td>1.787562</td>\n",
       "      <td>0.624951</td>\n",
       "      <td>-1.362902</td>\n",
       "      <td>2.156031</td>\n",
       "      <td>2.361684</td>\n",
       "      <td>-0.318203</td>\n",
       "      <td>-0.507173</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.879978</td>\n",
       "      <td>0.511523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1662</th>\n",
       "      <td>10794</td>\n",
       "      <td>0.600209</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-1.409524</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>0.294871</td>\n",
       "      <td>-0.656555</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.618179</td>\n",
       "      <td>2.789742</td>\n",
       "      <td>2.819853</td>\n",
       "      <td>-0.156738</td>\n",
       "      <td>-0.088907</td>\n",
       "      <td>2.016805</td>\n",
       "      <td>3.211106</td>\n",
       "      <td>-0.853464</td>\n",
       "      <td>0.905608</td>\n",
       "      <td>0.301516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1663</th>\n",
       "      <td>10798</td>\n",
       "      <td>-0.991692</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>0.357513</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>0.667909</td>\n",
       "      <td>0.050762</td>\n",
       "      <td>0.132465</td>\n",
       "      <td>-0.086936</td>\n",
       "      <td>0.549561</td>\n",
       "      <td>0.308705</td>\n",
       "      <td>0.289106</td>\n",
       "      <td>0.364771</td>\n",
       "      <td>0.277694</td>\n",
       "      <td>-0.819004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1664</th>\n",
       "      <td>10800</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.678701</td>\n",
       "      <td>-0.374615</td>\n",
       "      <td>-0.081159</td>\n",
       "      <td>-0.328916</td>\n",
       "      <td>-1.037220</td>\n",
       "      <td>-0.378396</td>\n",
       "      <td>-0.043254</td>\n",
       "      <td>-0.396120</td>\n",
       "      <td>-1.144724</td>\n",
       "      <td>-0.728384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1665</th>\n",
       "      <td>10802</td>\n",
       "      <td>0.241673</td>\n",
       "      <td>1.143014</td>\n",
       "      <td>-0.349302</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>0.705575</td>\n",
       "      <td>-0.594824</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>1.651388</td>\n",
       "      <td>0.164477</td>\n",
       "      <td>-0.042706</td>\n",
       "      <td>0.257420</td>\n",
       "      <td>-0.351806</td>\n",
       "      <td>0.906086</td>\n",
       "      <td>0.508372</td>\n",
       "      <td>0.295967</td>\n",
       "      <td>2.058919</td>\n",
       "      <td>-0.160213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1666</th>\n",
       "      <td>10804</td>\n",
       "      <td>-0.647497</td>\n",
       "      <td>-0.874880</td>\n",
       "      <td>-0.702709</td>\n",
       "      <td>-0.172768</td>\n",
       "      <td>-0.253396</td>\n",
       "      <td>-0.486480</td>\n",
       "      <td>-0.819258</td>\n",
       "      <td>-0.772763</td>\n",
       "      <td>0.609824</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.708962</td>\n",
       "      <td>2.712528</td>\n",
       "      <td>3.106109</td>\n",
       "      <td>0.797221</td>\n",
       "      <td>-0.708597</td>\n",
       "      <td>3.950342</td>\n",
       "      <td>4.360518</td>\n",
       "      <td>1.153993</td>\n",
       "      <td>-1.734194</td>\n",
       "      <td>-0.843457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1667 rows  374 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0       AGE  PTGENDER  PTEDUCAT  PTETHCAT  PTRACCAT   PTMARRY  \\\n",
       "0              0  0.069575 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "1              1  1.073477 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "2              5 -0.905643 -0.874880 -2.116339  4.591811 -0.253396 -0.486480   \n",
       "3             10 -0.016473 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "4             15  0.944404  1.143014 -1.056117 -0.172768 -0.253396 -0.486480   \n",
       "5             20  0.227331 -0.874880 -2.116339  4.591811  1.366533 -0.486480   \n",
       "6             22  1.532403  1.143014  0.710920 -0.172768 -0.253396  0.780795   \n",
       "7             24  0.012210  1.143014 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "8             28  0.671916  1.143014 -1.409524  4.591811 -0.253396  2.048069   \n",
       "9             33  1.001770 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "10            36 -1.206814 -0.874880 -2.469746 -0.172768  2.986463 -0.486480   \n",
       "11            41 -0.102522  1.143014  0.710920 -0.172768 -0.253396  2.048069   \n",
       "12            45 -0.174229  1.143014  0.710920 -0.172768  2.986463  2.048069   \n",
       "13            49 -1.522326 -0.874880  0.357513 -0.172768  2.986463  2.048069   \n",
       "14            53 -0.303302 -0.874880 -0.702709 -0.172768  2.986463  0.780795   \n",
       "15            60 -1.393253 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "16            64  0.557185  1.143014  0.710920 -0.172768 -0.253396  0.780795   \n",
       "17            69  1.360306 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "18            71  0.442453 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "19            75  0.428112 -0.874880 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "20            78 -0.088181 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "21            83 -0.418034  1.143014 -0.702709 -0.172768 -0.253396 -0.486480   \n",
       "22            84 -0.145546 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "23            93  0.342063 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "24            98  1.718842  1.143014  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "25            99  1.733184 -0.874880  0.710920 -0.172768 -0.253396  3.315344   \n",
       "26           101  1.561086 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "27           105  0.542843 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "28           106 -1.049058 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "29           112  0.901380 -0.874880 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "1637       10662  0.413770  1.143014  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1638       10667 -0.762229  1.143014 -1.409524 -0.172768 -0.253396  2.048069   \n",
       "1639       10670  0.356404  1.143014  1.064327 -0.172768 -0.253396 -0.486480   \n",
       "1640       10678  0.456795 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "1641       10696 -0.647497 -0.874880  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "1642       10702 -1.163790  1.143014  0.710920 -0.172768 -0.253396  0.780795   \n",
       "1643       10705  0.442453 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1644       10709  0.815331  1.143014  1.417734 -0.172768 -0.253396  0.780795   \n",
       "1645       10715 -0.561449  1.143014  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1646       10718 -1.780472  1.143014  0.004105  4.591811 -0.253396 -0.486480   \n",
       "1647       10722 -1.235497  1.143014  0.710920 -0.172768 -0.253396  3.315344   \n",
       "1648       10728  0.729282  1.143014  0.710920 -0.172768 -0.253396 -0.486480   \n",
       "1649       10743  0.327721 -0.874880 -0.349302 -0.172768 -0.253396  0.780795   \n",
       "1650       10746  0.628892 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1651       10750  1.245574  1.143014 -1.409524 -0.172768 -0.253396  2.048069   \n",
       "1652       10753 -0.905643  1.143014 -1.409524 -0.172768 -0.253396  2.048069   \n",
       "1653       10755  0.600209 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1654       10758  0.571526  1.143014 -0.349302 -0.172768 -0.253396 -0.486480   \n",
       "1655       10761  0.055234  1.143014 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "1656       10766  0.930062 -0.874880  1.064327 -0.172768 -0.253396 -0.486480   \n",
       "1657       10775 -1.106424  1.143014  0.004105  4.591811 -0.253396 -0.486480   \n",
       "1658       10776 -2.483203 -0.874880 -0.702709  4.591811 -0.253396 -0.486480   \n",
       "1659       10780  0.585868  1.143014 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "1660       10785 -1.135107 -0.874880  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "1661       10791  2.335525 -0.874880  1.417734 -0.172768 -0.253396 -0.486480   \n",
       "1662       10794  0.600209  1.143014 -1.409524 -0.172768 -0.253396 -0.486480   \n",
       "1663       10798 -0.991692 -0.874880  0.357513 -0.172768 -0.253396 -0.486480   \n",
       "1664       10800  0.729282  1.143014  0.004105 -0.172768 -0.253396 -0.486480   \n",
       "1665       10802  0.241673  1.143014 -0.349302 -0.172768 -0.253396 -0.486480   \n",
       "1666       10804 -0.647497 -0.874880 -0.702709 -0.172768 -0.253396 -0.486480   \n",
       "\n",
       "         APOE4     CDRSB      MMSE                    ...                      \\\n",
       "0    -0.819258 -0.772763  0.356549                    ...                       \n",
       "1     0.705575  0.828688 -1.669659                    ...                       \n",
       "2    -0.819258 -0.416885  0.103273                    ...                       \n",
       "3    -0.819258 -0.772763  0.609824                    ...                       \n",
       "4    -0.819258 -0.594824 -0.403279                    ...                       \n",
       "5     0.705575  1.362505 -1.669659                    ...                       \n",
       "6    -0.819258 -0.772763  0.356549                    ...                       \n",
       "7     0.705575  1.006627 -0.656555                    ...                       \n",
       "8    -0.819258 -0.772763  0.609824                    ...                       \n",
       "9     0.705575 -0.772763  0.609824                    ...                       \n",
       "10    0.705575 -0.772763  0.356549                    ...                       \n",
       "11   -0.819258 -0.772763  0.609824                    ...                       \n",
       "12   -0.819258 -0.772763  0.863100                    ...                       \n",
       "13    0.705575 -0.772763  0.609824                    ...                       \n",
       "14   -0.819258 -0.772763 -0.150003                    ...                       \n",
       "15    0.705575  0.650749 -1.416383                    ...                       \n",
       "16   -0.819258 -0.772763  0.863100                    ...                       \n",
       "17   -0.819258  0.116932  0.356549                    ...                       \n",
       "18   -0.819258 -0.772763  0.863100                    ...                       \n",
       "19   -0.819258 -0.416885 -0.403279                    ...                       \n",
       "20   -0.819258 -0.772763  0.609824                    ...                       \n",
       "21    2.230409 -0.061007 -0.403279                    ...                       \n",
       "22   -0.819258 -0.594824  0.863100                    ...                       \n",
       "23   -0.819258 -0.772763  0.609824                    ...                       \n",
       "24   -0.819258 -0.061007  0.058874                    ...                       \n",
       "25    0.705575 -0.416885  0.609824                    ...                       \n",
       "26   -0.819258 -0.772763  0.863100                    ...                       \n",
       "27   -0.819258  1.006627 -1.669659                    ...                       \n",
       "28    2.230409 -0.416885  0.103273                    ...                       \n",
       "29   -0.819258  0.650749 -0.909831                    ...                       \n",
       "...        ...       ...       ...                    ...                       \n",
       "1637 -0.819258 -0.772763 -0.150003                    ...                       \n",
       "1638 -0.819258 -0.772763  0.863100                    ...                       \n",
       "1639  0.705575 -0.594824  0.609824                    ...                       \n",
       "1640  0.705575 -0.594824  0.356549                    ...                       \n",
       "1641 -0.819258  1.184566 -0.656555                    ...                       \n",
       "1642  0.705575 -0.772763  0.863100                    ...                       \n",
       "1643 -0.819258 -0.594824  0.863100                    ...                       \n",
       "1644 -0.819258 -0.772763  0.863100                    ...                       \n",
       "1645  0.705575 -0.772763  0.863100                    ...                       \n",
       "1646  0.705575  1.362505 -0.656555                    ...                       \n",
       "1647 -0.819258 -0.772763  0.356549                    ...                       \n",
       "1648 -0.819258 -0.772763  0.863100                    ...                       \n",
       "1649 -0.819258 -0.772763  0.609824                    ...                       \n",
       "1650 -0.819258 -0.772763  0.609824                    ...                       \n",
       "1651 -0.819258 -0.594824  0.103273                    ...                       \n",
       "1652 -0.819258 -0.772763  0.609824                    ...                       \n",
       "1653 -0.819258  1.006627 -0.403279                    ...                       \n",
       "1654 -0.819258 -0.772763  0.103273                    ...                       \n",
       "1655 -0.819258 -0.061007 -0.150003                    ...                       \n",
       "1656  0.705575 -0.772763  0.609824                    ...                       \n",
       "1657  0.705575  0.116932 -0.150003                    ...                       \n",
       "1658 -0.819258  2.074260 -0.403279                    ...                       \n",
       "1659 -0.819258 -0.594824  0.609824                    ...                       \n",
       "1660 -0.819258 -0.772763  0.356549                    ...                       \n",
       "1661 -0.819258 -0.772763  0.863100                    ...                       \n",
       "1662  0.705575  0.294871 -0.656555                    ...                       \n",
       "1663  0.705575 -0.772763  0.609824                    ...                       \n",
       "1664 -0.819258 -0.594824  0.609824                    ...                       \n",
       "1665  0.705575 -0.594824  0.609824                    ...                       \n",
       "1666 -0.819258 -0.772763  0.609824                    ...                       \n",
       "\n",
       "      ST97TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                       1.439562   \n",
       "1                                      -0.118874   \n",
       "2                                      -0.164266   \n",
       "3                                       0.440952   \n",
       "4                                       0.486343   \n",
       "5                                      -0.542527   \n",
       "6                                      -0.875397   \n",
       "7                                      -0.875397   \n",
       "8                                       0.213995   \n",
       "9                                      -0.315570   \n",
       "10                                     -1.359571   \n",
       "11                                     -1.117484   \n",
       "12                                     -0.845136   \n",
       "13                                      2.528954   \n",
       "14                                     -1.874007   \n",
       "15                                     -1.329310   \n",
       "16                                      0.017300   \n",
       "17                                      0.319909   \n",
       "18                                     -0.708962   \n",
       "19                                      0.002169   \n",
       "20                                      1.651388   \n",
       "21                                     -0.179396   \n",
       "22                                      0.350169   \n",
       "23                                     -0.845136   \n",
       "24                                     -1.041832   \n",
       "25                                     -1.541137   \n",
       "26                                     -0.391223   \n",
       "27                                     -0.542527   \n",
       "28                                     -1.329310   \n",
       "29                                     -0.194527   \n",
       "...                                          ...   \n",
       "1637                                    1.076431   \n",
       "1638                                    0.879735   \n",
       "1639                                    0.743561   \n",
       "1640                                    0.244256   \n",
       "1641                                    0.894866   \n",
       "1642                                    1.015909   \n",
       "1643                                    0.304778   \n",
       "1644                                    1.515214   \n",
       "1645                                    0.032430   \n",
       "1646                                    0.985648   \n",
       "1647                                   -0.058353   \n",
       "1648                                    0.213995   \n",
       "1649                                    0.683039   \n",
       "1650                                    0.168604   \n",
       "1651                                   -0.497136   \n",
       "1652                                   -0.875397   \n",
       "1653                                   -0.587918   \n",
       "1654                                   -0.179396   \n",
       "1655                                    1.424431   \n",
       "1656                                   -0.724092   \n",
       "1657                                   -0.724092   \n",
       "1658                                    0.410691   \n",
       "1659                                    0.077821   \n",
       "1660                                    0.909996   \n",
       "1661                                    1.787562   \n",
       "1662                                   -0.618179   \n",
       "1663                                    0.667909   \n",
       "1664                                   -0.678701   \n",
       "1665                                    1.651388   \n",
       "1666                                   -0.708962   \n",
       "\n",
       "      ST98CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                      -0.547293   \n",
       "1                                      -0.763491   \n",
       "2                                      -0.231419   \n",
       "3                                       1.340932   \n",
       "4                                      -1.610034   \n",
       "5                                      -0.717163   \n",
       "6                                      -1.108847   \n",
       "7                                      -1.497723   \n",
       "8                                      -1.671805   \n",
       "9                                      -0.340922   \n",
       "10                                      0.157457   \n",
       "11                                     -0.799992   \n",
       "12                                      0.436830   \n",
       "13                                     -1.365758   \n",
       "14                                     -0.013817   \n",
       "15                                      0.106917   \n",
       "16                                     -0.555716   \n",
       "17                                     -0.505177   \n",
       "18                                     -1.385412   \n",
       "19                                     -0.079799   \n",
       "20                                      0.083051   \n",
       "21                                      0.153246   \n",
       "22                                     -0.704528   \n",
       "23                                      0.556161   \n",
       "24                                     -1.216946   \n",
       "25                                      0.987153   \n",
       "26                                     -0.868783   \n",
       "27                                     -1.895023   \n",
       "28                                     -0.239842   \n",
       "29                                     -0.321268   \n",
       "...                                          ...   \n",
       "1637                                    0.446658   \n",
       "1638                                    1.263719   \n",
       "1639                                    1.624517   \n",
       "1640                                    1.858966   \n",
       "1641                                    0.587046   \n",
       "1642                                    0.682510   \n",
       "1643                                    1.919333   \n",
       "1644                                   -0.916515   \n",
       "1645                                   -0.126128   \n",
       "1646                                    0.867823   \n",
       "1647                                    1.051732   \n",
       "1648                                    0.429811   \n",
       "1649                                    0.717607   \n",
       "1650                                    1.430781   \n",
       "1651                                    0.800437   \n",
       "1652                                   -0.047510   \n",
       "1653                                    0.222036   \n",
       "1654                                   -0.046106   \n",
       "1655                                   -1.469645   \n",
       "1656                                    1.436396   \n",
       "1657                                   -1.392432   \n",
       "1658                                    0.328731   \n",
       "1659                                   -0.865975   \n",
       "1660                                    1.770521   \n",
       "1661                                    0.624951   \n",
       "1662                                    2.789742   \n",
       "1663                                    0.050762   \n",
       "1664                                   -0.374615   \n",
       "1665                                    0.164477   \n",
       "1666                                    2.712528   \n",
       "\n",
       "      ST98SA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                      -0.209333   \n",
       "1                                       0.269184   \n",
       "2                                       0.243549   \n",
       "3                                       0.841696   \n",
       "4                                      -1.837147   \n",
       "5                                      -0.200788   \n",
       "6                                      -1.106553   \n",
       "7                                      -1.239000   \n",
       "8                                      -1.606433   \n",
       "9                                      -0.269148   \n",
       "10                                     -0.046979   \n",
       "11                                     -1.029649   \n",
       "12                                      0.328999   \n",
       "13                                     -1.858509   \n",
       "14                                      0.427266   \n",
       "15                                      0.333271   \n",
       "16                                     -0.756210   \n",
       "17                                     -1.140733   \n",
       "18                                     -1.166368   \n",
       "19                                     -0.170881   \n",
       "20                                      0.111103   \n",
       "21                                     -0.179426   \n",
       "22                                      0.713522   \n",
       "23                                      0.393086   \n",
       "24                                     -1.251817   \n",
       "25                                      0.769064   \n",
       "26                                     -0.363142   \n",
       "27                                      0.153827   \n",
       "28                                      0.141010   \n",
       "29                                      0.000018   \n",
       "...                                          ...   \n",
       "1637                                    0.175190   \n",
       "1638                                    0.593893   \n",
       "1639                                    1.033958   \n",
       "1640                                   -0.435774   \n",
       "1641                                    1.247581   \n",
       "1642                                    0.273457   \n",
       "1643                                    0.615255   \n",
       "1644                                   -0.226423   \n",
       "1645                                    0.542623   \n",
       "1646                                    0.158100   \n",
       "1647                                   -0.175153   \n",
       "1648                                   -0.525496   \n",
       "1649                                    0.418721   \n",
       "1650                                    0.482808   \n",
       "1651                                   -0.145246   \n",
       "1652                                    0.512715   \n",
       "1653                                   -0.811752   \n",
       "1654                                   -0.303328   \n",
       "1655                                   -1.896961   \n",
       "1656                                   -0.358870   \n",
       "1657                                   -0.628036   \n",
       "1658                                    1.452660   \n",
       "1659                                   -0.717758   \n",
       "1660                                    1.405663   \n",
       "1661                                   -1.362902   \n",
       "1662                                    2.819853   \n",
       "1663                                    0.132465   \n",
       "1664                                   -0.081159   \n",
       "1665                                   -0.042706   \n",
       "1666                                    3.106109   \n",
       "\n",
       "      ST98TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                      -0.836143   \n",
       "1                                      -1.464361   \n",
       "2                                      -0.356837   \n",
       "3                                       1.332369   \n",
       "4                                      -0.538322   \n",
       "5                                      -0.575550   \n",
       "6                                       0.080588   \n",
       "7                                      -0.380104   \n",
       "8                                      -0.519708   \n",
       "9                                      -0.505748   \n",
       "10                                      1.211379   \n",
       "11                                      0.248113   \n",
       "12                                      0.815835   \n",
       "13                                      1.132270   \n",
       "14                                     -0.100897   \n",
       "15                                     -0.072976   \n",
       "16                                      0.117816   \n",
       "17                                      0.680885   \n",
       "18                                     -0.314956   \n",
       "19                                      0.164350   \n",
       "20                                      0.206232   \n",
       "21                                      0.918211   \n",
       "22                                     -2.041390   \n",
       "23                                      0.727419   \n",
       "24                                     -0.142778   \n",
       "25                                      0.387717   \n",
       "26                                     -0.468520   \n",
       "27                                     -2.688221   \n",
       "28                                     -0.631391   \n",
       "29                                      0.201578   \n",
       "...                                          ...   \n",
       "1637                                    0.224845   \n",
       "1638                                    1.192765   \n",
       "1639                                    0.601776   \n",
       "1640                                    2.798209   \n",
       "1641                                   -0.594164   \n",
       "1642                                    0.210885   \n",
       "1643                                    1.979200   \n",
       "1644                                   -1.096737   \n",
       "1645                                   -1.157232   \n",
       "1646                                    0.629696   \n",
       "1647                                    1.620884   \n",
       "1648                                    0.299301   \n",
       "1649                                    0.052667   \n",
       "1650                                    1.136924   \n",
       "1651                                    1.825636   \n",
       "1652                                   -0.468520   \n",
       "1653                                    0.383063   \n",
       "1654                                    0.704152   \n",
       "1655                                    0.103855   \n",
       "1656                                    1.848903   \n",
       "1657                                   -1.659806   \n",
       "1658                                   -1.036242   \n",
       "1659                                    0.029400   \n",
       "1660                                    0.834449   \n",
       "1661                                    2.156031   \n",
       "1662                                   -0.156738   \n",
       "1663                                   -0.086936   \n",
       "1664                                   -0.328916   \n",
       "1665                                    0.257420   \n",
       "1666                                    0.797221   \n",
       "\n",
       "      ST98TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                      -0.023183   \n",
       "1                                      -0.492644   \n",
       "2                                      -1.328287   \n",
       "3                                      -0.370584   \n",
       "4                                      -0.445698   \n",
       "5                                      -0.342417   \n",
       "6                                      -0.680429   \n",
       "7                                      -2.023090   \n",
       "8                                       0.671621   \n",
       "9                                       0.173991   \n",
       "10                                      0.793681   \n",
       "11                                     -0.652262   \n",
       "12                                     -0.708597   \n",
       "13                                     -1.131113   \n",
       "14                                     -1.778970   \n",
       "15                                     -0.689818   \n",
       "16                                      0.934520   \n",
       "17                                      0.699789   \n",
       "18                                     -1.666299   \n",
       "19                                     -0.436309   \n",
       "20                                     -0.173410   \n",
       "21                                     -0.624094   \n",
       "22                                     -1.609964   \n",
       "23                                      0.117656   \n",
       "24                                     -0.502034   \n",
       "25                                      0.493225   \n",
       "26                                     -0.520812   \n",
       "27                                     -1.685078   \n",
       "28                                     -0.210967   \n",
       "29                                     -0.633483   \n",
       "...                                          ...   \n",
       "1637                                   -0.699208   \n",
       "1638                                    0.399333   \n",
       "1639                                    0.803070   \n",
       "1640                                    2.981373   \n",
       "1641                                    0.718567   \n",
       "1642                                   -0.361195   \n",
       "1643                                    1.826497   \n",
       "1644                                   -0.905771   \n",
       "1645                                   -0.830657   \n",
       "1646                                    0.887573   \n",
       "1647                                    0.774903   \n",
       "1648                                    1.141083   \n",
       "1649                                    0.859406   \n",
       "1650                                   -0.257914   \n",
       "1651                                   -0.013793   \n",
       "1652                                    0.990855   \n",
       "1653                                    1.037801   \n",
       "1654                                    0.183381   \n",
       "1655                                   -0.173410   \n",
       "1656                                    2.117563   \n",
       "1657                                   -1.215616   \n",
       "1658                                    0.136434   \n",
       "1659                                   -0.079518   \n",
       "1660                                   -0.924549   \n",
       "1661                                    2.361684   \n",
       "1662                                   -0.088907   \n",
       "1663                                    0.549561   \n",
       "1664                                   -1.037220   \n",
       "1665                                   -0.351806   \n",
       "1666                                   -0.708597   \n",
       "\n",
       "      ST99CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                       3.176360   \n",
       "1                                       0.169014   \n",
       "2                                       0.605692   \n",
       "3                                       0.845894   \n",
       "4                                      -0.254604   \n",
       "5                                      -0.579416   \n",
       "6                                       0.215578   \n",
       "7                                       0.067936   \n",
       "8                                       0.229206   \n",
       "9                                      -0.054720   \n",
       "10                                     -0.669136   \n",
       "11                                     -0.803717   \n",
       "12                                     -0.503323   \n",
       "13                                     -0.574305   \n",
       "14                                      0.480765   \n",
       "15                                      0.598310   \n",
       "16                                      0.099736   \n",
       "17                                     -0.000206   \n",
       "18                                      1.083824   \n",
       "19                                      0.354134   \n",
       "20                                     -0.623140   \n",
       "21                                      0.533007   \n",
       "22                                     -0.887191   \n",
       "23                                      0.131536   \n",
       "24                                     -1.280713   \n",
       "25                                     -0.193844   \n",
       "26                                      0.673267   \n",
       "27                                     -1.823580   \n",
       "28                                      0.048061   \n",
       "29                                     -0.288675   \n",
       "...                                          ...   \n",
       "1637                                    1.122438   \n",
       "1638                                    0.860658   \n",
       "1639                                    0.513132   \n",
       "1640                                    0.839647   \n",
       "1641                                   -0.408492   \n",
       "1642                                    1.741397   \n",
       "1643                                    0.517107   \n",
       "1644                                   -0.129109   \n",
       "1645                                    0.296781   \n",
       "1646                                   -0.042795   \n",
       "1647                                    0.310409   \n",
       "1648                                    0.057147   \n",
       "1649                                    1.270079   \n",
       "1650                                    1.207616   \n",
       "1651                                    0.071911   \n",
       "1652                                    0.321198   \n",
       "1653                                   -0.805989   \n",
       "1654                                    0.915172   \n",
       "1655                                   -1.645842   \n",
       "1656                                    0.225799   \n",
       "1657                                   -0.414170   \n",
       "1658                                   -0.231890   \n",
       "1659                                   -1.177932   \n",
       "1660                                    1.195691   \n",
       "1661                                   -0.318203   \n",
       "1662                                    2.016805   \n",
       "1663                                    0.308705   \n",
       "1664                                   -0.378396   \n",
       "1665                                    0.906086   \n",
       "1666                                    3.950342   \n",
       "\n",
       "      ST99SA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                       2.627168   \n",
       "1                                       0.420666   \n",
       "2                                       1.071538   \n",
       "3                                       0.584538   \n",
       "4                                      -0.814145   \n",
       "5                                      -0.389462   \n",
       "6                                       0.921514   \n",
       "7                                       0.540685   \n",
       "8                                      -0.202510   \n",
       "9                                       0.538377   \n",
       "10                                     -0.696434   \n",
       "11                                     -0.885694   \n",
       "12                                     -1.019562   \n",
       "13                                     -1.054182   \n",
       "14                                      0.556841   \n",
       "15                                      1.895514   \n",
       "16                                      0.146007   \n",
       "17                                      0.302955   \n",
       "18                                      1.161552   \n",
       "19                                      0.572997   \n",
       "20                                     -0.280984   \n",
       "21                                      0.425282   \n",
       "22                                     -0.200202   \n",
       "23                                     -0.387154   \n",
       "24                                     -1.104960   \n",
       "25                                      0.249869   \n",
       "26                                      0.669936   \n",
       "27                                      2.052462   \n",
       "28                                      0.328343   \n",
       "29                                     -0.380230   \n",
       "...                                          ...   \n",
       "1637                                    1.064613   \n",
       "1638                                    0.886893   \n",
       "1639                                    0.076765   \n",
       "1640                                   -0.112495   \n",
       "1641                                   -1.169585   \n",
       "1642                                    0.302955   \n",
       "1643                                   -0.574107   \n",
       "1644                                   -0.733363   \n",
       "1645                                    0.725329   \n",
       "1646                                   -0.647965   \n",
       "1647                                   -0.280984   \n",
       "1648                                   -1.391159   \n",
       "1649                                    0.441438   \n",
       "1650                                    0.566073   \n",
       "1651                                   -0.964168   \n",
       "1652                                   -0.050178   \n",
       "1653                                   -0.673353   \n",
       "1654                                    0.406817   \n",
       "1655                                   -1.850462   \n",
       "1656                                   -0.426391   \n",
       "1657                                   -0.114803   \n",
       "1658                                    0.582230   \n",
       "1659                                   -1.979713   \n",
       "1660                                    0.349116   \n",
       "1661                                   -0.507173   \n",
       "1662                                    3.211106   \n",
       "1663                                    0.289106   \n",
       "1664                                   -0.043254   \n",
       "1665                                    0.508372   \n",
       "1666                                    4.360518   \n",
       "\n",
       "      ST99TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                       1.421115   \n",
       "1                                      -0.533728   \n",
       "2                                      -0.027816   \n",
       "3                                       0.947581   \n",
       "4                                       0.903061   \n",
       "5                                      -0.614673   \n",
       "6                                      -0.307079   \n",
       "7                                      -0.222086   \n",
       "8                                       0.700697   \n",
       "9                                      -0.647052   \n",
       "10                                      0.235258   \n",
       "11                                     -0.027816   \n",
       "12                                      1.408973   \n",
       "13                                      0.368819   \n",
       "14                                      0.239305   \n",
       "15                                     -0.800849   \n",
       "16                                     -0.108762   \n",
       "17                                     -0.173518   \n",
       "18                                      0.372866   \n",
       "19                                     -0.019721   \n",
       "20                                     -0.234228   \n",
       "21                                      0.271684   \n",
       "22                                     -1.007261   \n",
       "23                                      1.105426   \n",
       "24                                     -0.954646   \n",
       "25                                     -0.651099   \n",
       "26                                      0.243353   \n",
       "27                                     -4.378655   \n",
       "28                                      0.113839   \n",
       "29                                     -0.060194   \n",
       "...                                          ...   \n",
       "1637                                    0.372866   \n",
       "1638                                    0.231211   \n",
       "1639                                   -0.007580   \n",
       "1640                                    0.870683   \n",
       "1641                                    0.704744   \n",
       "1642                                    1.384689   \n",
       "1643                                    1.558723   \n",
       "1644                                    0.429528   \n",
       "1645                                   -0.493255   \n",
       "1646                                    0.304062   \n",
       "1647                                    0.575231   \n",
       "1648                                    1.485871   \n",
       "1649                                    0.449765   \n",
       "1650                                    0.777595   \n",
       "1651                                    0.563089   \n",
       "1652                                    1.101379   \n",
       "1653                                   -0.800849   \n",
       "1654                                    0.554994   \n",
       "1655                                   -1.173200   \n",
       "1656                                    0.134076   \n",
       "1657                                   -0.808943   \n",
       "1658                                   -0.974882   \n",
       "1659                                    0.660224   \n",
       "1660                                    1.089237   \n",
       "1661                                    0.016704   \n",
       "1662                                   -0.853464   \n",
       "1663                                    0.364771   \n",
       "1664                                   -0.396120   \n",
       "1665                                    0.295967   \n",
       "1666                                    1.153993   \n",
       "\n",
       "      ST99TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \\\n",
       "0                                      -0.606512   \n",
       "1                                      -0.132373   \n",
       "2                                      -0.837174   \n",
       "3                                      -0.465551   \n",
       "4                                       0.393025   \n",
       "5                                      -0.580883   \n",
       "6                                      -0.529624   \n",
       "7                                      -1.093465   \n",
       "8                                       0.226435   \n",
       "9                                      -0.196445   \n",
       "10                                     -1.016578   \n",
       "11                                     -1.131909   \n",
       "12                                     -1.285684   \n",
       "13                                     -0.247704   \n",
       "14                                      0.008588   \n",
       "15                                     -0.709028   \n",
       "16                                      0.636502   \n",
       "17                                     -0.286147   \n",
       "18                                     -0.568068   \n",
       "19                                     -0.683399   \n",
       "20                                     -1.260055   \n",
       "21                                     -0.606512   \n",
       "22                                     -0.298962   \n",
       "23                                     -0.555253   \n",
       "24                                     -0.004227   \n",
       "25                                     -0.401479   \n",
       "26                                      1.059382   \n",
       "27                                     -1.887969   \n",
       "28                                     -1.285684   \n",
       "29                                      0.726204   \n",
       "...                                          ...   \n",
       "1637                                   -1.055022   \n",
       "1638                                   -0.388664   \n",
       "1639                                    0.713389   \n",
       "1640                                    1.020939   \n",
       "1641                                    0.226435   \n",
       "1642                                    1.776998   \n",
       "1643                                    0.380210   \n",
       "1644                                   -0.017042   \n",
       "1645                                   -0.939691   \n",
       "1646                                    1.149084   \n",
       "1647                                    0.303323   \n",
       "1648                                    1.033753   \n",
       "1649                                    2.340839   \n",
       "1650                                    1.225972   \n",
       "1651                                    2.071733   \n",
       "1652                                   -0.286147   \n",
       "1653                                   -0.068300   \n",
       "1654                                    1.200343   \n",
       "1655                                    0.700574   \n",
       "1656                                    1.828257   \n",
       "1657                                    0.405839   \n",
       "1658                                   -1.541975   \n",
       "1659                                    0.585243   \n",
       "1660                                    0.508356   \n",
       "1661                                    0.879978   \n",
       "1662                                    0.905608   \n",
       "1663                                    0.277694   \n",
       "1664                                   -1.144724   \n",
       "1665                                    2.058919   \n",
       "1666                                   -1.734194   \n",
       "\n",
       "      ST9SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16  \n",
       "0                                      3.287650  \n",
       "1                                      1.716909  \n",
       "2                                     -1.111001  \n",
       "3                                     -0.701054  \n",
       "4                                     -1.546838  \n",
       "5                                     -0.826196  \n",
       "6                                     -0.017811  \n",
       "7                                     -1.531016  \n",
       "8                                     -1.171414  \n",
       "9                                     -0.795989  \n",
       "10                                    -0.161652  \n",
       "11                                    -0.760029  \n",
       "12                                    -0.888047  \n",
       "13                                    -1.292240  \n",
       "14                                    -0.163090  \n",
       "15                                     1.738485  \n",
       "16                                     0.081439  \n",
       "17                                     0.084316  \n",
       "18                                    -1.344023  \n",
       "19                                    -1.420258  \n",
       "20                                    -1.095178  \n",
       "21                                     0.533099  \n",
       "22                                    -0.761468  \n",
       "23                                    -0.915377  \n",
       "24                                    -1.185798  \n",
       "25                                     0.771875  \n",
       "26                                     2.620229  \n",
       "27                                     1.072502  \n",
       "28                                     0.222403  \n",
       "29                                     1.229289  \n",
       "...                                         ...  \n",
       "1637                                  -0.991613  \n",
       "1638                                   0.314461  \n",
       "1639                                   0.691324  \n",
       "1640                                   0.894140  \n",
       "1641                                  -1.371352  \n",
       "1642                                  -0.748522  \n",
       "1643                                   0.344668  \n",
       "1644                                   0.674063  \n",
       "1645                                  -0.377413  \n",
       "1646                                  -0.683794  \n",
       "1647                                   0.021026  \n",
       "1648                                   0.491386  \n",
       "1649                                   0.284255  \n",
       "1650                                   2.146993  \n",
       "1651                                  -0.767221  \n",
       "1652                                  -0.293985  \n",
       "1653                                   1.049488  \n",
       "1654                                  -0.232134  \n",
       "1655                                   0.430972  \n",
       "1656                                  -0.994490  \n",
       "1657                                   0.311585  \n",
       "1658                                  -1.886302  \n",
       "1659                                   1.117093  \n",
       "1660                                  -0.380290  \n",
       "1661                                   0.511523  \n",
       "1662                                   0.301516  \n",
       "1663                                  -0.819004  \n",
       "1664                                  -0.728384  \n",
       "1665                                  -0.160213  \n",
       "1666                                  -0.843457  \n",
       "\n",
       "[1667 rows x 374 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.loc[held_idx].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> working on iteration 0 of 19 <<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:6: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 1 timepoints ahead... *****\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "152 features selected\n",
      "validation prediction accuracy is 88.3400422841 percent \n",
      " p = 0.0 \n",
      " r = 0.939893835941\n",
      "testing prediction accuracy is 81.7253045417 percent \n",
      " p = 0.0 \n",
      " r = 0.904020489489\n",
      "***** 2 timepoints ahead... *****\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "152 features selected\n",
      "validation prediction accuracy is 88.3400422841 percent \n",
      " p = 0.0 \n",
      " r = 0.939893835941\n",
      "testing prediction accuracy is nan percent \n",
      " p = 1.0 \n",
      " r = nan\n",
      "***** 3 timepoints ahead... *****\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "152 features selected\n",
      "validation prediction accuracy is 88.3400422841 percent \n",
      " p = 0.0 \n",
      " r = 0.939893835941\n",
      "testing prediction accuracy is nan percent \n",
      " p = 1.0 \n",
      " r = nan\n",
      "***** 4 timepoints ahead... *****\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-202a3bc5ed2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                ])\n\u001b[1;32m     16\u001b[0m         \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheld_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_feature_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prediction_%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mii\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Science/scripts/kfold_learning.py\u001b[0m in \u001b[0;36mkfold_feature_learning\u001b[0;34m(train, test, y, t_y, clf, problem, folds, scale, verbose, search, p_cutoff, regcols, regdf, keep_cols, out_dir, output, save_int, vote, weighted)\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_clf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msig_mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig_mtx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msig_mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coef_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1190\u001b[0m                 for train, test in folds)\n\u001b[1;32m   1191\u001b[0m         mse_paths = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0;32m-> 1192\u001b[0;31m                              backend=\"threading\")(jobs)\n\u001b[0m\u001b[1;32m   1193\u001b[0m         \u001b[0mmse_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_l1_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m         \u001b[0mmean_mse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmse_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py\u001b[0m in \u001b[0;36m_path_residuals\u001b[0;34m(X, y, train, test, path, path_params, alphas, l1_ratio, X_order, dtype)\u001b[0m\n\u001b[1;32m    989\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_scale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXy\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    990\u001b[0m         _pre_fit(X_train, y_train, None, precompute, normalize, fit_intercept,\n\u001b[0;32m--> 991\u001b[0;31m                  copy=False)\n\u001b[0m\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m     \u001b[0mpath_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36m_pre_fit\u001b[0;34m(X, y, Xy, precompute, normalize, fit_intercept, copy)\u001b[0m\n\u001b[1;32m    549\u001b[0m         precompute = np.empty(shape=(n_features, n_features), dtype=X.dtype,\n\u001b[1;32m    550\u001b[0m                               order='C')\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecompute\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__array__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = pandas.DataFrame(np.zeros((20,1)),columns = ['n'])\n",
    "weight_mate = {}\n",
    "for i in range(max(list(tps.values()))):\n",
    "    weight_mate.update({i: {}})\n",
    "    print('>'*10,'working on iteration %s of %s'%(i,max(list(tps.values()))),'<'*10)\n",
    "    results.ix[i,'n'] = len(np.array(list(tps.values()))[np.array(list(tps.values()))>(i+1)])\n",
    "    held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]\n",
    "    train = xdf.drop(xdf.index[held_idx], axis=0)\n",
    "    y = ydf.loc[train.index]['y_ADAS13']\n",
    "    test = xdf.loc[held_idx]\n",
    "    t_y = ydf.loc[test]['y_ADAS13']\n",
    "    \n",
    "    \n",
    "    \n",
    "    tdf = ydf.loc[held_idx]\n",
    "    for ii in range(1,max(list(tps.values()))-(i+1)):\n",
    "        print('*'*5,ii,'timepoints ahead...','*'*5)\n",
    "        t_y = pandas.Series([ydf.loc[ydf[ydf.RID==x\n",
    "                                        ].index[i+ii]]['y_ADAS13'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "        test = pandas.DataFrame(xdf.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]\n",
    "        output = kfl.kfold_feature_learning(train, test, y, t_y, scale = False, weighted = True)\n",
    "        results.ix[i,'validation_%s'%(ii)] = output[0][0]\n",
    "        results.ix[i,'prediction_%s'%(ii)] = output[0][1]\n",
    "        weight_mate[i].update({ii: output[1]})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> working on iteration 0 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "22 features selected\n",
      "validation prediction accuracy is 90.0389860449 percent \n",
      " p = 0.0 \n",
      " r = 0.948888750302\n",
      "testing prediction accuracy is 87.3361832142 percent \n",
      " p = 0.0 \n",
      " r = 0.934538298916\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      "***** 17 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 1 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "13 features selected\n",
      "validation prediction accuracy is 89.6805795416 percent \n",
      " p = 0.0 \n",
      " r = 0.946998308032\n",
      "testing prediction accuracy is 90.6601906724 percent \n",
      " p = 0.0 \n",
      " r = 0.95215645076\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 2 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "14 features selected\n",
      "validation prediction accuracy is 90.1555232824 percent \n",
      " p = 0.0 \n",
      " r = 0.949502623916\n",
      "testing prediction accuracy is 87.7757216082 percent \n",
      " p = 0.0 \n",
      " r = 0.936886981488\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 3 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "11 features selected\n",
      "validation prediction accuracy is 89.9710172166 percent \n",
      " p = 0.0 \n",
      " r = 0.94853053307\n",
      "testing prediction accuracy is 88.173534516 percent \n",
      " p = 0.0 \n",
      " r = 0.939007638499\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 4 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "10 features selected\n",
      "validation prediction accuracy is 89.5778171656 percent \n",
      " p = 0.0 \n",
      " r = 0.946455583562\n",
      "testing prediction accuracy is 90.9726724491 percent \n",
      " p = 0.0 \n",
      " r = 0.953795955376\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 5 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "10 features selected\n",
      "validation prediction accuracy is 89.5105937992 percent \n",
      " p = 0.0 \n",
      " r = 0.946100384733\n",
      "testing prediction accuracy is 90.2700562533 percent \n",
      " p = 0.0 \n",
      " r = 0.950105553364\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 6 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "10 features selected\n",
      "validation prediction accuracy is 89.5356891719 percent \n",
      " p = 0.0 \n",
      " r = 0.946233000756\n",
      "testing prediction accuracy is 91.7226323372 percent \n",
      " p = 0.0 \n",
      " r = 0.957719334342\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 7 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "10 features selected\n",
      "validation prediction accuracy is 89.7070321934 percent \n",
      " p = 0.0 \n",
      " r = 0.947137963517\n",
      "testing prediction accuracy is 88.4270302005 percent \n",
      " p = 1.54603104378e-217 \n",
      " r = 0.940356476026\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 8 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "13 features selected\n",
      "validation prediction accuracy is 89.7686475929 percent \n",
      " p = 0.0 \n",
      " r = 0.947463179194\n",
      "testing prediction accuracy is 88.4392946757 percent \n",
      " p = 1.02933221953e-178 \n",
      " r = 0.940421685605\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 9 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "13 features selected\n",
      "validation prediction accuracy is 89.5552117437 percent \n",
      " p = 0.0 \n",
      " r = 0.946336154565\n",
      "testing prediction accuracy is 92.8356404454 percent \n",
      " p = 1.92659076448e-159 \n",
      " r = 0.963512534664\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 10 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "14 features selected\n",
      "validation prediction accuracy is 89.5978899664 percent \n",
      " p = 0.0 \n",
      " r = 0.946561619581\n",
      "testing prediction accuracy is 92.4780779446 percent \n",
      " p = 5.78372938236e-138 \n",
      " r = 0.961655228991\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 11 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "13 features selected\n",
      "validation prediction accuracy is 89.6724955759 percent \n",
      " p = 0.0 \n",
      " r = 0.946955625021\n",
      "testing prediction accuracy is 89.2235091951 percent \n",
      " p = 6.48659480414e-108 \n",
      " r = 0.944581966772\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 12 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "13 features selected\n",
      "validation prediction accuracy is 89.6490821912 percent \n",
      " p = 0.0 \n",
      " r = 0.946831992442\n",
      "testing prediction accuracy is 90.1840431352 percent \n",
      " p = 3.41449128281e-96 \n",
      " r = 0.949652795158\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 13 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "12 features selected\n",
      "validation prediction accuracy is 89.6241814457 percent \n",
      " p = 0.0 \n",
      " r = 0.946700488252\n",
      "testing prediction accuracy is 93.6914466845 percent \n",
      " p = 6.42027179207e-98 \n",
      " r = 0.967943421304\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 14 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "12 features selected\n",
      "validation prediction accuracy is 89.6925858186 percent \n",
      " p = 0.0 \n",
      " r = 0.947061697138\n",
      "testing prediction accuracy is 87.4166089905 percent \n",
      " p = 2.66751368828e-56 \n",
      " r = 0.934968496745\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 15 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "11 features selected\n",
      "validation prediction accuracy is 89.7445835137 percent \n",
      " p = 0.0 \n",
      " r = 0.947336178522\n",
      "testing prediction accuracy is 91.368312654 percent \n",
      " p = 8.84360826557e-44 \n",
      " r = 0.955867734857\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 16 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "11 features selected\n",
      "validation prediction accuracy is 89.6967137762 percent \n",
      " p = 0.0 \n",
      " r = 0.947083490386\n",
      "testing prediction accuracy is 89.3262738926 percent \n",
      " p = 4.33970861165e-18 \n",
      " r = 0.945125779421\n",
      "***** 1 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 17 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "12 features selected\n",
      "validation prediction accuracy is 89.6905275887 percent \n",
      " p = 0.0 \n",
      " r = 0.947050830678\n",
      "testing prediction accuracy is 75.7966552245 percent \n",
      " p = 0.00104584707665 \n",
      " r = 0.870612745281\n",
      ">>>>>>>>>> working on iteration 18 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "12 features selected\n",
      "validation prediction accuracy is 89.6564551599 percent \n",
      " p = 0.0 \n",
      " r = 0.946870926578\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 745)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-bc33d6404248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheld_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mydf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_ADAS13'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_feature_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_same_tp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/jakevogel/git/hack_projects/kfold_learning.py\u001b[0m in \u001b[0;36mkfold_feature_learning\u001b[0;34m(train, test, y, t_y, clf, problem, folds, scale, verbose, search, p_cutoff, regcols, regdf, keep_cols, out_dir, output, save_int, vote, weighted)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mntest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py3/lib/python3.5/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    414\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 416\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 745)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "results = pandas.DataFrame(np.zeros((20,1)),columns = ['n'])\n",
    "weight_mate = pandas.DataFrame(np.zeros((20,xdf.shape[-1])))\n",
    "for i in range(max(list(tps.values()))):\n",
    "    print('>'*10,'working on iteration %s of %s'%(i,max(list(tps.values()))),'<'*10)\n",
    "    results.ix[i,'n'] = len(np.array(list(tps.values()))[np.array(list(tps.values()))>(i+1)])\n",
    "    held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]\n",
    "    train = xdf.drop(xdf.index[held_idx], axis=0)\n",
    "    y = ydf.loc[train.index]['y_ADAS13']\n",
    "    test = xdf.loc[held_idx]\n",
    "    t_y = ydf.loc[test.index]['y_ADAS13']\n",
    "    output = kfl.kfold_feature_learning(train, test, y, t_y, scale = False, weighted = True)\n",
    "    results.ix[i,'validation'] = output[0][0]\n",
    "    results.ix[i,'test_same_tp'] = output[0][1]\n",
    "    weight_mate.loc[i] = output[1]\n",
    "    \n",
    "    tdf = ydf.loc[held_idx]\n",
    "    for ii in range(1,max(list(tps.values()))-(i+1)):\n",
    "        print('*'*5,ii,'timepoints ahead...','*'*5)\n",
    "        t_y = pandas.Series([ydf.loc[ydf[ydf.RID==x\n",
    "                                        ].index[i+ii]]['y_ADAS13'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "        test = pandas.DataFrame(xdf.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]\n",
    "        ntest = check_array(test,accept_sparse='csr')\n",
    "        t_predicted = pandas.Series(safe_sparse_dot(ntest,np.array(output[1]).T,dense_output=True),index=test.index)\n",
    "        r,p = stats.pearsonr(t_y[test.index],t_predicted)\n",
    "        \n",
    "        results.ix[i,'prediction_%s'%(ii)] = (r**2)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>validation</th>\n",
       "      <th>test_same_tp</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>prediction_6</th>\n",
       "      <th>prediction_7</th>\n",
       "      <th>prediction_8</th>\n",
       "      <th>prediction_9</th>\n",
       "      <th>prediction_10</th>\n",
       "      <th>prediction_11</th>\n",
       "      <th>prediction_12</th>\n",
       "      <th>prediction_13</th>\n",
       "      <th>prediction_14</th>\n",
       "      <th>prediction_15</th>\n",
       "      <th>prediction_16</th>\n",
       "      <th>prediction_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1667.0</td>\n",
       "      <td>88.340042</td>\n",
       "      <td>87.601281</td>\n",
       "      <td>81.725305</td>\n",
       "      <td>78.675230</td>\n",
       "      <td>77.971875</td>\n",
       "      <td>77.552741</td>\n",
       "      <td>70.214987</td>\n",
       "      <td>66.503169</td>\n",
       "      <td>58.029984</td>\n",
       "      <td>50.880613</td>\n",
       "      <td>45.667146</td>\n",
       "      <td>49.006550</td>\n",
       "      <td>48.499513</td>\n",
       "      <td>47.723063</td>\n",
       "      <td>53.466512</td>\n",
       "      <td>50.809928</td>\n",
       "      <td>48.132222</td>\n",
       "      <td>40.848266</td>\n",
       "      <td>50.802742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1590.0</td>\n",
       "      <td>88.029616</td>\n",
       "      <td>89.554582</td>\n",
       "      <td>81.902839</td>\n",
       "      <td>80.396980</td>\n",
       "      <td>80.625941</td>\n",
       "      <td>75.847305</td>\n",
       "      <td>73.655176</td>\n",
       "      <td>67.863707</td>\n",
       "      <td>60.711688</td>\n",
       "      <td>54.020016</td>\n",
       "      <td>59.261154</td>\n",
       "      <td>59.929171</td>\n",
       "      <td>57.863543</td>\n",
       "      <td>60.080036</td>\n",
       "      <td>52.316123</td>\n",
       "      <td>55.065547</td>\n",
       "      <td>48.016969</td>\n",
       "      <td>63.752387</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1496.0</td>\n",
       "      <td>88.274814</td>\n",
       "      <td>87.673861</td>\n",
       "      <td>83.198060</td>\n",
       "      <td>81.235273</td>\n",
       "      <td>73.938407</td>\n",
       "      <td>72.067768</td>\n",
       "      <td>64.820222</td>\n",
       "      <td>62.420851</td>\n",
       "      <td>55.477288</td>\n",
       "      <td>59.814971</td>\n",
       "      <td>58.868421</td>\n",
       "      <td>56.370232</td>\n",
       "      <td>61.406813</td>\n",
       "      <td>53.929409</td>\n",
       "      <td>59.703425</td>\n",
       "      <td>48.034166</td>\n",
       "      <td>60.989083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1348.0</td>\n",
       "      <td>88.157155</td>\n",
       "      <td>87.431034</td>\n",
       "      <td>82.977186</td>\n",
       "      <td>75.572777</td>\n",
       "      <td>73.172637</td>\n",
       "      <td>68.733242</td>\n",
       "      <td>66.835217</td>\n",
       "      <td>60.634825</td>\n",
       "      <td>65.908572</td>\n",
       "      <td>65.353614</td>\n",
       "      <td>61.736679</td>\n",
       "      <td>65.173333</td>\n",
       "      <td>54.724020</td>\n",
       "      <td>59.745880</td>\n",
       "      <td>50.628223</td>\n",
       "      <td>65.201271</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107.0</td>\n",
       "      <td>88.040709</td>\n",
       "      <td>86.422801</td>\n",
       "      <td>79.123045</td>\n",
       "      <td>77.225404</td>\n",
       "      <td>72.123232</td>\n",
       "      <td>65.152768</td>\n",
       "      <td>61.299394</td>\n",
       "      <td>67.373060</td>\n",
       "      <td>67.242661</td>\n",
       "      <td>65.836853</td>\n",
       "      <td>68.379768</td>\n",
       "      <td>57.692059</td>\n",
       "      <td>59.416867</td>\n",
       "      <td>47.610646</td>\n",
       "      <td>56.971250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>942.0</td>\n",
       "      <td>87.907602</td>\n",
       "      <td>87.427174</td>\n",
       "      <td>81.021046</td>\n",
       "      <td>70.830861</td>\n",
       "      <td>69.465895</td>\n",
       "      <td>64.397786</td>\n",
       "      <td>70.946573</td>\n",
       "      <td>70.460121</td>\n",
       "      <td>67.862380</td>\n",
       "      <td>74.824704</td>\n",
       "      <td>64.108903</td>\n",
       "      <td>64.826620</td>\n",
       "      <td>54.975324</td>\n",
       "      <td>71.751209</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>670.0</td>\n",
       "      <td>87.946724</td>\n",
       "      <td>87.330922</td>\n",
       "      <td>78.119071</td>\n",
       "      <td>68.865914</td>\n",
       "      <td>62.450124</td>\n",
       "      <td>66.801322</td>\n",
       "      <td>70.915200</td>\n",
       "      <td>70.844873</td>\n",
       "      <td>72.677773</td>\n",
       "      <td>64.850794</td>\n",
       "      <td>69.481401</td>\n",
       "      <td>56.171503</td>\n",
       "      <td>70.241429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>462.0</td>\n",
       "      <td>88.004570</td>\n",
       "      <td>86.450607</td>\n",
       "      <td>78.879595</td>\n",
       "      <td>72.422665</td>\n",
       "      <td>74.748042</td>\n",
       "      <td>75.446893</td>\n",
       "      <td>76.355675</td>\n",
       "      <td>76.510297</td>\n",
       "      <td>66.796467</td>\n",
       "      <td>69.005156</td>\n",
       "      <td>55.390964</td>\n",
       "      <td>71.028941</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>379.0</td>\n",
       "      <td>88.085954</td>\n",
       "      <td>86.651306</td>\n",
       "      <td>82.555468</td>\n",
       "      <td>76.841584</td>\n",
       "      <td>75.497361</td>\n",
       "      <td>73.983180</td>\n",
       "      <td>76.235010</td>\n",
       "      <td>64.501719</td>\n",
       "      <td>66.985624</td>\n",
       "      <td>57.341289</td>\n",
       "      <td>66.567022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>277.0</td>\n",
       "      <td>87.905782</td>\n",
       "      <td>89.466359</td>\n",
       "      <td>83.830951</td>\n",
       "      <td>82.271121</td>\n",
       "      <td>80.024456</td>\n",
       "      <td>82.462006</td>\n",
       "      <td>70.405961</td>\n",
       "      <td>70.304283</td>\n",
       "      <td>57.160707</td>\n",
       "      <td>72.042792</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>244.0</td>\n",
       "      <td>87.934751</td>\n",
       "      <td>90.108772</td>\n",
       "      <td>87.350691</td>\n",
       "      <td>83.910439</td>\n",
       "      <td>84.677865</td>\n",
       "      <td>74.108073</td>\n",
       "      <td>71.541904</td>\n",
       "      <td>57.996110</td>\n",
       "      <td>75.569087</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>221.0</td>\n",
       "      <td>87.995022</td>\n",
       "      <td>89.421106</td>\n",
       "      <td>85.023838</td>\n",
       "      <td>86.030202</td>\n",
       "      <td>77.089707</td>\n",
       "      <td>73.552954</td>\n",
       "      <td>60.632338</td>\n",
       "      <td>75.791262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>189.0</td>\n",
       "      <td>87.982231</td>\n",
       "      <td>88.831825</td>\n",
       "      <td>87.705324</td>\n",
       "      <td>81.431696</td>\n",
       "      <td>76.575137</td>\n",
       "      <td>61.842657</td>\n",
       "      <td>76.904842</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>162.0</td>\n",
       "      <td>88.028914</td>\n",
       "      <td>89.421499</td>\n",
       "      <td>83.185286</td>\n",
       "      <td>78.933829</td>\n",
       "      <td>66.586334</td>\n",
       "      <td>80.486070</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>123.0</td>\n",
       "      <td>88.041578</td>\n",
       "      <td>88.334451</td>\n",
       "      <td>81.648237</td>\n",
       "      <td>68.463962</td>\n",
       "      <td>82.143166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>81.0</td>\n",
       "      <td>88.080999</td>\n",
       "      <td>88.196318</td>\n",
       "      <td>83.629093</td>\n",
       "      <td>82.492674</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>88.032789</td>\n",
       "      <td>87.317702</td>\n",
       "      <td>82.268563</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>88.021922</td>\n",
       "      <td>78.737649</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n  validation  test_same_tp  prediction_1  prediction_2  \\\n",
       "0   1667.0   88.340042     87.601281     81.725305     78.675230   \n",
       "1   1590.0   88.029616     89.554582     81.902839     80.396980   \n",
       "2   1496.0   88.274814     87.673861     83.198060     81.235273   \n",
       "3   1348.0   88.157155     87.431034     82.977186     75.572777   \n",
       "4   1107.0   88.040709     86.422801     79.123045     77.225404   \n",
       "5    942.0   87.907602     87.427174     81.021046     70.830861   \n",
       "6    670.0   87.946724     87.330922     78.119071     68.865914   \n",
       "7    462.0   88.004570     86.450607     78.879595     72.422665   \n",
       "8    379.0   88.085954     86.651306     82.555468     76.841584   \n",
       "9    277.0   87.905782     89.466359     83.830951     82.271121   \n",
       "10   244.0   87.934751     90.108772     87.350691     83.910439   \n",
       "11   221.0   87.995022     89.421106     85.023838     86.030202   \n",
       "12   189.0   87.982231     88.831825     87.705324     81.431696   \n",
       "13   162.0   88.028914     89.421499     83.185286     78.933829   \n",
       "14   123.0   88.041578     88.334451     81.648237     68.463962   \n",
       "15    81.0   88.080999     88.196318     83.629093     82.492674   \n",
       "16    36.0   88.032789     87.317702     82.268563           NaN   \n",
       "17    10.0   88.021922     78.737649           NaN           NaN   \n",
       "18     0.0         NaN           NaN           NaN           NaN   \n",
       "19     0.0         NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_3  prediction_4  prediction_5  prediction_6  prediction_7  \\\n",
       "0      77.971875     77.552741     70.214987     66.503169     58.029984   \n",
       "1      80.625941     75.847305     73.655176     67.863707     60.711688   \n",
       "2      73.938407     72.067768     64.820222     62.420851     55.477288   \n",
       "3      73.172637     68.733242     66.835217     60.634825     65.908572   \n",
       "4      72.123232     65.152768     61.299394     67.373060     67.242661   \n",
       "5      69.465895     64.397786     70.946573     70.460121     67.862380   \n",
       "6      62.450124     66.801322     70.915200     70.844873     72.677773   \n",
       "7      74.748042     75.446893     76.355675     76.510297     66.796467   \n",
       "8      75.497361     73.983180     76.235010     64.501719     66.985624   \n",
       "9      80.024456     82.462006     70.405961     70.304283     57.160707   \n",
       "10     84.677865     74.108073     71.541904     57.996110     75.569087   \n",
       "11     77.089707     73.552954     60.632338     75.791262           NaN   \n",
       "12     76.575137     61.842657     76.904842           NaN           NaN   \n",
       "13     66.586334     80.486070           NaN           NaN           NaN   \n",
       "14     82.143166           NaN           NaN           NaN           NaN   \n",
       "15           NaN           NaN           NaN           NaN           NaN   \n",
       "16           NaN           NaN           NaN           NaN           NaN   \n",
       "17           NaN           NaN           NaN           NaN           NaN   \n",
       "18           NaN           NaN           NaN           NaN           NaN   \n",
       "19           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_8  prediction_9  prediction_10  prediction_11  prediction_12  \\\n",
       "0      50.880613     45.667146      49.006550      48.499513      47.723063   \n",
       "1      54.020016     59.261154      59.929171      57.863543      60.080036   \n",
       "2      59.814971     58.868421      56.370232      61.406813      53.929409   \n",
       "3      65.353614     61.736679      65.173333      54.724020      59.745880   \n",
       "4      65.836853     68.379768      57.692059      59.416867      47.610646   \n",
       "5      74.824704     64.108903      64.826620      54.975324      71.751209   \n",
       "6      64.850794     69.481401      56.171503      70.241429            NaN   \n",
       "7      69.005156     55.390964      71.028941            NaN            NaN   \n",
       "8      57.341289     66.567022            NaN            NaN            NaN   \n",
       "9      72.042792           NaN            NaN            NaN            NaN   \n",
       "10           NaN           NaN            NaN            NaN            NaN   \n",
       "11           NaN           NaN            NaN            NaN            NaN   \n",
       "12           NaN           NaN            NaN            NaN            NaN   \n",
       "13           NaN           NaN            NaN            NaN            NaN   \n",
       "14           NaN           NaN            NaN            NaN            NaN   \n",
       "15           NaN           NaN            NaN            NaN            NaN   \n",
       "16           NaN           NaN            NaN            NaN            NaN   \n",
       "17           NaN           NaN            NaN            NaN            NaN   \n",
       "18           NaN           NaN            NaN            NaN            NaN   \n",
       "19           NaN           NaN            NaN            NaN            NaN   \n",
       "\n",
       "    prediction_13  prediction_14  prediction_15  prediction_16  prediction_17  \n",
       "0       53.466512      50.809928      48.132222      40.848266      50.802742  \n",
       "1       52.316123      55.065547      48.016969      63.752387            NaN  \n",
       "2       59.703425      48.034166      60.989083            NaN            NaN  \n",
       "3       50.628223      65.201271            NaN            NaN            NaN  \n",
       "4       56.971250            NaN            NaN            NaN            NaN  \n",
       "5             NaN            NaN            NaN            NaN            NaN  \n",
       "6             NaN            NaN            NaN            NaN            NaN  \n",
       "7             NaN            NaN            NaN            NaN            NaN  \n",
       "8             NaN            NaN            NaN            NaN            NaN  \n",
       "9             NaN            NaN            NaN            NaN            NaN  \n",
       "10            NaN            NaN            NaN            NaN            NaN  \n",
       "11            NaN            NaN            NaN            NaN            NaN  \n",
       "12            NaN            NaN            NaN            NaN            NaN  \n",
       "13            NaN            NaN            NaN            NaN            NaN  \n",
       "14            NaN            NaN            NaN            NaN            NaN  \n",
       "15            NaN            NaN            NaN            NaN            NaN  \n",
       "16            NaN            NaN            NaN            NaN            NaN  \n",
       "17            NaN            NaN            NaN            NaN            NaN  \n",
       "18            NaN            NaN            NaN            NaN            NaN  \n",
       "19            NaN            NaN            NaN            NaN            NaN  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMSE', 'ADAS13_bl', 'RAVLT_immediate', 'CDRSB', 'FAQ',\n",
       "       'ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16', 'RAVLT_learning',\n",
       "       'RAVLT_perc_forgetting', 'ST40CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.columns[[9,25,10,8,14,219,11,13,217]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9      0.263619\n",
       "25     0.184841\n",
       "10     0.146337\n",
       "8      0.105768\n",
       "14     0.027075\n",
       "219    0.018623\n",
       "11     0.002464\n",
       "13     0.001271\n",
       "217    0.000578\n",
       "191    0.000210\n",
       "23     0.000201\n",
       "193    0.000174\n",
       "34     0.000167\n",
       "2      0.000139\n",
       "12     0.000138\n",
       "26     0.000110\n",
       "364    0.000104\n",
       "40     0.000100\n",
       "344    0.000095\n",
       "39     0.000086\n",
       "265    0.000080\n",
       "100    0.000080\n",
       "224    0.000075\n",
       "86     0.000074\n",
       "280    0.000072\n",
       "77     0.000068\n",
       "3      0.000065\n",
       "220    0.000063\n",
       "351    0.000063\n",
       "293    0.000062\n",
       "         ...   \n",
       "115    0.000000\n",
       "140    0.000000\n",
       "142    0.000000\n",
       "273    0.000000\n",
       "156    0.000000\n",
       "167    0.000000\n",
       "166    0.000000\n",
       "165    0.000000\n",
       "274    0.000000\n",
       "275    0.000000\n",
       "162    0.000000\n",
       "161    0.000000\n",
       "160    0.000000\n",
       "159    0.000000\n",
       "278    0.000000\n",
       "157    0.000000\n",
       "155    0.000000\n",
       "284    0.000000\n",
       "154    0.000000\n",
       "153    0.000000\n",
       "152    0.000000\n",
       "282    0.000000\n",
       "283    0.000000\n",
       "149    0.000000\n",
       "148    0.000000\n",
       "147    0.000000\n",
       "146    0.000000\n",
       "145    0.000000\n",
       "144    0.000000\n",
       "113    0.000000\n",
       "Length: 374, dtype: float64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_mate.mean(axis=0).abs().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_years = pandas.DataFrame((xdf.values.transpose() * xdf.Years_bl.values).transpose(), columns = xdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_years = pandas.DataFrame((ydf[ydf.columns[-4:]].values.transpose() * xdf.Years_bl.values).transpose(), \n",
    "                           columns = ydf.columns[-4:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_years['RID'] = ydf['RID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> working on iteration 0 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "3 features selected\n",
      "validation prediction accuracy is 29.4735473146 percent \n",
      " p = 0.0 \n",
      " r = 0.542895453238\n",
      "testing prediction accuracy is 59.6735699192 percent \n",
      " p = 0.0 \n",
      " r = 0.772486698392\n",
      "***** 1 timepoints ahead... *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:12: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      "***** 17 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 1 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "3 features selected\n",
      "validation prediction accuracy is 33.1709769163 percent \n",
      " p = 0.0 \n",
      " r = 0.575942505085\n",
      "testing prediction accuracy is 74.3158996428 percent \n",
      " p = 0.0 \n",
      " r = 0.862066700684\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 2 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "3 features selected\n",
      "validation prediction accuracy is 32.8883758847 percent \n",
      " p = 0.0 \n",
      " r = 0.573483878454\n",
      "testing prediction accuracy is 72.389087187 percent \n",
      " p = 0.0 \n",
      " r = 0.850817766546\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 3 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "3 features selected\n",
      "validation prediction accuracy is 31.843225463 percent \n",
      " p = 0.0 \n",
      " r = 0.564298019339\n",
      "testing prediction accuracy is 70.7019635884 percent \n",
      " p = 0.0 \n",
      " r = 0.84084459675\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 4 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 31.2704339491 percent \n",
      " p = 0.0 \n",
      " r = 0.559199731304\n",
      "testing prediction accuracy is 71.3167270946 percent \n",
      " p = 6.21250616936e-302 \n",
      " r = 0.844492315504\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 5 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 32.4483523376 percent \n",
      " p = 0.0 \n",
      " r = 0.569634552478\n",
      "testing prediction accuracy is 85.0481222101 percent \n",
      " p = 0.0 \n",
      " r = 0.922215388128\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 6 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 33.7089080416 percent \n",
      " p = 0.0 \n",
      " r = 0.580593730948\n",
      "testing prediction accuracy is 87.1714788904 percent \n",
      " p = 4.467750023e-300 \n",
      " r = 0.933656676142\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 7 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 34.4869871655 percent \n",
      " p = 0.0 \n",
      " r = 0.587256223173\n",
      "testing prediction accuracy is 86.6968107182 percent \n",
      " p = 1.29123896415e-203 \n",
      " r = 0.931111221704\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 8 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 35.3874446202 percent \n",
      " p = 0.0 \n",
      " r = 0.59487347075\n",
      "testing prediction accuracy is 86.5925355626 percent \n",
      " p = 1.41061384567e-166 \n",
      " r = 0.930551103178\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 9 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 34.8256258109 percent \n",
      " p = 0.0 \n",
      " r = 0.590132407269\n",
      "testing prediction accuracy is 89.327814033 percent \n",
      " p = 1.23321497024e-135 \n",
      " r = 0.945133927192\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 10 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 34.9442580829 percent \n",
      " p = 0.0 \n",
      " r = 0.591136685403\n",
      "testing prediction accuracy is 91.9605600837 percent \n",
      " p = 1.81948512565e-134 \n",
      " r = 0.958960687848\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 11 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 34.5053358355 percent \n",
      " p = 0.0 \n",
      " r = 0.587412426116\n",
      "testing prediction accuracy is 90.7489730814 percent \n",
      " p = 3.54731877833e-115 \n",
      " r = 0.952622554223\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 12 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 34.2871064197 percent \n",
      " p = 0.0 \n",
      " r = 0.585551931255\n",
      "testing prediction accuracy is 89.3364620803 percent \n",
      " p = 7.91450256244e-93 \n",
      " r = 0.945179676465\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 13 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 36.1812912362 percent \n",
      " p = 0.0 \n",
      " r = 0.601508863078\n",
      "testing prediction accuracy is 90.4147605405 percent \n",
      " p = 2.23375443286e-83 \n",
      " r = 0.950866765328\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 14 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 36.2902036202 percent \n",
      " p = 0.0 \n",
      " r = 0.602413509312\n",
      "testing prediction accuracy is 88.4706235828 percent \n",
      " p = 1.33365535201e-58 \n",
      " r = 0.940588239257\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 15 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 33.2837371842 percent \n",
      " p = 0.0 \n",
      " r = 0.576920594052\n",
      "testing prediction accuracy is 88.3835730152 percent \n",
      " p = 1.11745716339e-38 \n",
      " r = 0.940125380017\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 16 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 36.7415153195 percent \n",
      " p = 0.0 \n",
      " r = 0.606147798144\n",
      "testing prediction accuracy is 86.3363133416 percent \n",
      " p = 2.93545371463e-16 \n",
      " r = 0.92917336026\n",
      "***** 1 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 17 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 38.8873648427 percent \n",
      " p = 0.0 \n",
      " r = 0.623597344788\n",
      "testing prediction accuracy is 80.5964033163 percent \n",
      " p = 0.000421929439099 \n",
      " r = 0.897754996178\n",
      ">>>>>>>>>> working on iteration 18 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 35.6384482071 percent \n",
      " p = 0.0 \n",
      " r = 0.596979465368\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 374)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-183-458b47760ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_years\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheld_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_years\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_ADAS13'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_feature_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0myrs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0myrs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_same_tp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Science/scripts/kfold_learning.py\u001b[0m in \u001b[0;36mkfold_feature_learning\u001b[0;34m(train, test, y, t_y, clf, problem, folds, scale, verbose, search, p_cutoff, regcols, regdf, keep_cols, out_dir, output, save_int, vote, weighted)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mntest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 431\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 374)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "yrs_results = pandas.DataFrame(np.zeros((20,1)),columns = ['n'])\n",
    "yrs_weight_mate = pandas.DataFrame(np.zeros((20,xdf.shape[-1])))\n",
    "for i in range(max(list(tps.values()))):\n",
    "    print('>'*10,'working on iteration %s of %s'%(i,max(list(tps.values()))),'<'*10)\n",
    "    results.ix[i,'n'] = len(np.array(list(tps.values()))[np.array(list(tps.values()))>(i+1)])\n",
    "    held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]\n",
    "    train = x_years.drop(x_years.index[held_idx], axis=0)\n",
    "    y = y_years.loc[train.index]['y_ADAS13']\n",
    "    test = x_years.loc[held_idx]\n",
    "    t_y = y_years.loc[test.index]['y_ADAS13']\n",
    "    output = kfl.kfold_feature_learning(train, test, y, t_y, scale = False, weighted = True)\n",
    "    yrs_results.ix[i,'validation'] = output[0][0]\n",
    "    yrs_results.ix[i,'test_same_tp'] = output[0][1]\n",
    "    yrs_weight_mate.loc[i] = output[1]\n",
    "    \n",
    "    tdf = y_years.loc[held_idx]\n",
    "    for ii in range(1,max(list(tps.values()))-(i+1)):\n",
    "        print('*'*5,ii,'timepoints ahead...','*'*5)\n",
    "        t_y = pandas.Series([y_years.loc[y_years[y_years.RID==x\n",
    "                                        ].index[i+ii]]['y_ADAS13'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "        test = pandas.DataFrame(x_years.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]\n",
    "        ntest = check_array(test,accept_sparse='csr')\n",
    "        t_predicted = pandas.Series(safe_sparse_dot(ntest,np.array(output[1]).T,dense_output=True),index=test.index)\n",
    "        r,p = stats.pearsonr(t_y[test.index],t_predicted)\n",
    "        \n",
    "        yrs_results.ix[i,'prediction_%s'%(ii)] = (r**2)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>validation</th>\n",
       "      <th>test_same_tp</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>prediction_6</th>\n",
       "      <th>prediction_7</th>\n",
       "      <th>prediction_8</th>\n",
       "      <th>prediction_9</th>\n",
       "      <th>prediction_10</th>\n",
       "      <th>prediction_11</th>\n",
       "      <th>prediction_12</th>\n",
       "      <th>prediction_13</th>\n",
       "      <th>prediction_14</th>\n",
       "      <th>prediction_15</th>\n",
       "      <th>prediction_16</th>\n",
       "      <th>prediction_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.473547</td>\n",
       "      <td>59.673570</td>\n",
       "      <td>52.431278</td>\n",
       "      <td>46.606180</td>\n",
       "      <td>30.361488</td>\n",
       "      <td>6.087687</td>\n",
       "      <td>1.439572</td>\n",
       "      <td>10.444066</td>\n",
       "      <td>9.220723</td>\n",
       "      <td>8.884591</td>\n",
       "      <td>8.834618</td>\n",
       "      <td>6.765131</td>\n",
       "      <td>6.052703</td>\n",
       "      <td>4.746801</td>\n",
       "      <td>5.173208</td>\n",
       "      <td>0.218401</td>\n",
       "      <td>16.159976</td>\n",
       "      <td>13.001290</td>\n",
       "      <td>38.703408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.170977</td>\n",
       "      <td>74.315900</td>\n",
       "      <td>61.508451</td>\n",
       "      <td>40.471236</td>\n",
       "      <td>7.553595</td>\n",
       "      <td>3.055517</td>\n",
       "      <td>17.900506</td>\n",
       "      <td>22.598590</td>\n",
       "      <td>22.455931</td>\n",
       "      <td>22.056961</td>\n",
       "      <td>25.960319</td>\n",
       "      <td>30.388360</td>\n",
       "      <td>27.926805</td>\n",
       "      <td>31.523611</td>\n",
       "      <td>24.589808</td>\n",
       "      <td>38.705769</td>\n",
       "      <td>35.377511</td>\n",
       "      <td>51.060832</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.888376</td>\n",
       "      <td>72.389087</td>\n",
       "      <td>42.400298</td>\n",
       "      <td>6.614416</td>\n",
       "      <td>3.119255</td>\n",
       "      <td>16.657094</td>\n",
       "      <td>16.718693</td>\n",
       "      <td>9.976980</td>\n",
       "      <td>5.323247</td>\n",
       "      <td>11.561171</td>\n",
       "      <td>12.044826</td>\n",
       "      <td>9.536724</td>\n",
       "      <td>13.385344</td>\n",
       "      <td>15.931107</td>\n",
       "      <td>33.210492</td>\n",
       "      <td>24.169759</td>\n",
       "      <td>20.995667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.843225</td>\n",
       "      <td>70.701964</td>\n",
       "      <td>10.874566</td>\n",
       "      <td>1.155477</td>\n",
       "      <td>9.205948</td>\n",
       "      <td>9.970846</td>\n",
       "      <td>3.644567</td>\n",
       "      <td>0.753441</td>\n",
       "      <td>1.367711</td>\n",
       "      <td>0.922844</td>\n",
       "      <td>0.621658</td>\n",
       "      <td>0.836612</td>\n",
       "      <td>0.511586</td>\n",
       "      <td>0.742548</td>\n",
       "      <td>1.044994</td>\n",
       "      <td>0.712129</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.270434</td>\n",
       "      <td>71.316727</td>\n",
       "      <td>4.022418</td>\n",
       "      <td>0.023178</td>\n",
       "      <td>0.059351</td>\n",
       "      <td>0.873473</td>\n",
       "      <td>2.541199</td>\n",
       "      <td>6.263413</td>\n",
       "      <td>9.825204</td>\n",
       "      <td>7.875591</td>\n",
       "      <td>5.453994</td>\n",
       "      <td>4.190468</td>\n",
       "      <td>9.077144</td>\n",
       "      <td>6.717687</td>\n",
       "      <td>2.624615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.448352</td>\n",
       "      <td>85.048122</td>\n",
       "      <td>22.080120</td>\n",
       "      <td>8.096878</td>\n",
       "      <td>12.609433</td>\n",
       "      <td>13.551085</td>\n",
       "      <td>30.891100</td>\n",
       "      <td>31.166841</td>\n",
       "      <td>31.927577</td>\n",
       "      <td>40.426300</td>\n",
       "      <td>33.816600</td>\n",
       "      <td>33.410336</td>\n",
       "      <td>28.354544</td>\n",
       "      <td>7.499293</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.708908</td>\n",
       "      <td>87.171479</td>\n",
       "      <td>28.504324</td>\n",
       "      <td>9.785388</td>\n",
       "      <td>8.227396</td>\n",
       "      <td>17.052265</td>\n",
       "      <td>25.953804</td>\n",
       "      <td>28.075822</td>\n",
       "      <td>31.059334</td>\n",
       "      <td>27.890222</td>\n",
       "      <td>32.632710</td>\n",
       "      <td>21.705227</td>\n",
       "      <td>35.893748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.486987</td>\n",
       "      <td>86.696811</td>\n",
       "      <td>46.882553</td>\n",
       "      <td>31.783368</td>\n",
       "      <td>38.606051</td>\n",
       "      <td>37.206486</td>\n",
       "      <td>41.444812</td>\n",
       "      <td>41.051973</td>\n",
       "      <td>34.939862</td>\n",
       "      <td>32.627739</td>\n",
       "      <td>27.140268</td>\n",
       "      <td>50.620408</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.387445</td>\n",
       "      <td>86.592536</td>\n",
       "      <td>67.198384</td>\n",
       "      <td>56.249239</td>\n",
       "      <td>46.996789</td>\n",
       "      <td>46.516662</td>\n",
       "      <td>46.450596</td>\n",
       "      <td>34.620023</td>\n",
       "      <td>35.232410</td>\n",
       "      <td>30.963226</td>\n",
       "      <td>37.459748</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.825626</td>\n",
       "      <td>89.327814</td>\n",
       "      <td>79.375974</td>\n",
       "      <td>69.325138</td>\n",
       "      <td>64.955696</td>\n",
       "      <td>65.710269</td>\n",
       "      <td>50.201364</td>\n",
       "      <td>48.421024</td>\n",
       "      <td>38.410945</td>\n",
       "      <td>47.993691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.944258</td>\n",
       "      <td>91.960560</td>\n",
       "      <td>84.537630</td>\n",
       "      <td>79.229054</td>\n",
       "      <td>77.855864</td>\n",
       "      <td>62.640564</td>\n",
       "      <td>58.269474</td>\n",
       "      <td>44.591906</td>\n",
       "      <td>65.931425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.505336</td>\n",
       "      <td>90.748973</td>\n",
       "      <td>83.203470</td>\n",
       "      <td>83.474333</td>\n",
       "      <td>72.288375</td>\n",
       "      <td>65.485851</td>\n",
       "      <td>49.686920</td>\n",
       "      <td>71.726042</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.287106</td>\n",
       "      <td>89.336462</td>\n",
       "      <td>87.141995</td>\n",
       "      <td>78.687638</td>\n",
       "      <td>71.444461</td>\n",
       "      <td>53.495897</td>\n",
       "      <td>72.225962</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.181291</td>\n",
       "      <td>90.414761</td>\n",
       "      <td>82.028420</td>\n",
       "      <td>74.769107</td>\n",
       "      <td>60.510751</td>\n",
       "      <td>76.917324</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.290204</td>\n",
       "      <td>88.470624</td>\n",
       "      <td>78.640933</td>\n",
       "      <td>60.795248</td>\n",
       "      <td>79.127865</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.283737</td>\n",
       "      <td>88.383573</td>\n",
       "      <td>82.158826</td>\n",
       "      <td>80.258368</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.741515</td>\n",
       "      <td>86.336313</td>\n",
       "      <td>79.161027</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38.887365</td>\n",
       "      <td>80.596403</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n  validation  test_same_tp  prediction_1  prediction_2  prediction_3  \\\n",
       "0   0.0   29.473547     59.673570     52.431278     46.606180     30.361488   \n",
       "1   0.0   33.170977     74.315900     61.508451     40.471236      7.553595   \n",
       "2   0.0   32.888376     72.389087     42.400298      6.614416      3.119255   \n",
       "3   0.0   31.843225     70.701964     10.874566      1.155477      9.205948   \n",
       "4   0.0   31.270434     71.316727      4.022418      0.023178      0.059351   \n",
       "5   0.0   32.448352     85.048122     22.080120      8.096878     12.609433   \n",
       "6   0.0   33.708908     87.171479     28.504324      9.785388      8.227396   \n",
       "7   0.0   34.486987     86.696811     46.882553     31.783368     38.606051   \n",
       "8   0.0   35.387445     86.592536     67.198384     56.249239     46.996789   \n",
       "9   0.0   34.825626     89.327814     79.375974     69.325138     64.955696   \n",
       "10  0.0   34.944258     91.960560     84.537630     79.229054     77.855864   \n",
       "11  0.0   34.505336     90.748973     83.203470     83.474333     72.288375   \n",
       "12  0.0   34.287106     89.336462     87.141995     78.687638     71.444461   \n",
       "13  0.0   36.181291     90.414761     82.028420     74.769107     60.510751   \n",
       "14  0.0   36.290204     88.470624     78.640933     60.795248     79.127865   \n",
       "15  0.0   33.283737     88.383573     82.158826     80.258368           NaN   \n",
       "16  0.0   36.741515     86.336313     79.161027           NaN           NaN   \n",
       "17  0.0   38.887365     80.596403           NaN           NaN           NaN   \n",
       "18  0.0         NaN           NaN           NaN           NaN           NaN   \n",
       "19  0.0         NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_4  prediction_5  prediction_6  prediction_7  prediction_8  \\\n",
       "0       6.087687      1.439572     10.444066      9.220723      8.884591   \n",
       "1       3.055517     17.900506     22.598590     22.455931     22.056961   \n",
       "2      16.657094     16.718693      9.976980      5.323247     11.561171   \n",
       "3       9.970846      3.644567      0.753441      1.367711      0.922844   \n",
       "4       0.873473      2.541199      6.263413      9.825204      7.875591   \n",
       "5      13.551085     30.891100     31.166841     31.927577     40.426300   \n",
       "6      17.052265     25.953804     28.075822     31.059334     27.890222   \n",
       "7      37.206486     41.444812     41.051973     34.939862     32.627739   \n",
       "8      46.516662     46.450596     34.620023     35.232410     30.963226   \n",
       "9      65.710269     50.201364     48.421024     38.410945     47.993691   \n",
       "10     62.640564     58.269474     44.591906     65.931425           NaN   \n",
       "11     65.485851     49.686920     71.726042           NaN           NaN   \n",
       "12     53.495897     72.225962           NaN           NaN           NaN   \n",
       "13     76.917324           NaN           NaN           NaN           NaN   \n",
       "14           NaN           NaN           NaN           NaN           NaN   \n",
       "15           NaN           NaN           NaN           NaN           NaN   \n",
       "16           NaN           NaN           NaN           NaN           NaN   \n",
       "17           NaN           NaN           NaN           NaN           NaN   \n",
       "18           NaN           NaN           NaN           NaN           NaN   \n",
       "19           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_9  prediction_10  prediction_11  prediction_12  prediction_13  \\\n",
       "0       8.834618       6.765131       6.052703       4.746801       5.173208   \n",
       "1      25.960319      30.388360      27.926805      31.523611      24.589808   \n",
       "2      12.044826       9.536724      13.385344      15.931107      33.210492   \n",
       "3       0.621658       0.836612       0.511586       0.742548       1.044994   \n",
       "4       5.453994       4.190468       9.077144       6.717687       2.624615   \n",
       "5      33.816600      33.410336      28.354544       7.499293            NaN   \n",
       "6      32.632710      21.705227      35.893748            NaN            NaN   \n",
       "7      27.140268      50.620408            NaN            NaN            NaN   \n",
       "8      37.459748            NaN            NaN            NaN            NaN   \n",
       "9            NaN            NaN            NaN            NaN            NaN   \n",
       "10           NaN            NaN            NaN            NaN            NaN   \n",
       "11           NaN            NaN            NaN            NaN            NaN   \n",
       "12           NaN            NaN            NaN            NaN            NaN   \n",
       "13           NaN            NaN            NaN            NaN            NaN   \n",
       "14           NaN            NaN            NaN            NaN            NaN   \n",
       "15           NaN            NaN            NaN            NaN            NaN   \n",
       "16           NaN            NaN            NaN            NaN            NaN   \n",
       "17           NaN            NaN            NaN            NaN            NaN   \n",
       "18           NaN            NaN            NaN            NaN            NaN   \n",
       "19           NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "    prediction_14  prediction_15  prediction_16  prediction_17  \n",
       "0        0.218401      16.159976      13.001290      38.703408  \n",
       "1       38.705769      35.377511      51.060832            NaN  \n",
       "2       24.169759      20.995667            NaN            NaN  \n",
       "3        0.712129            NaN            NaN            NaN  \n",
       "4             NaN            NaN            NaN            NaN  \n",
       "5             NaN            NaN            NaN            NaN  \n",
       "6             NaN            NaN            NaN            NaN  \n",
       "7             NaN            NaN            NaN            NaN  \n",
       "8             NaN            NaN            NaN            NaN  \n",
       "9             NaN            NaN            NaN            NaN  \n",
       "10            NaN            NaN            NaN            NaN  \n",
       "11            NaN            NaN            NaN            NaN  \n",
       "12            NaN            NaN            NaN            NaN  \n",
       "13            NaN            NaN            NaN            NaN  \n",
       "14            NaN            NaN            NaN            NaN  \n",
       "15            NaN            NaN            NaN            NaN  \n",
       "16            NaN            NaN            NaN            NaN  \n",
       "17            NaN            NaN            NaN            NaN  \n",
       "18            NaN            NaN            NaN            NaN  \n",
       "19            NaN            NaN            NaN            NaN  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['MMSE', 'ADAS13_bl', 'RAVLT_immediate', 'CDRSB',\n",
       "       'ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16', 'RAVLT_learning'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xdf.columns[[9,25,10,8,219,11,]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnk = abs(weight_mate.mean()).sort_values(ascending=False)\n",
    "xdf.columns[[x for x in range(len(jnk)) if jnk[x]>0.0001]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xdf.Years_bl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for_stats = xdf[['CDRSB', 'MMSE', 'RAVLT_immediate', 'RAVLT_learning',\n",
    "       'RAVLT_perc_forgetting', 'FAQ', 'ADAS13_bl',\n",
    "       'ST40TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16', 'Years_bl']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "for_stats['ADAS13'] = ydf['y_ADAS13']\n",
    "for_stats['RID'] = ydf['RID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for_stats.to_csv('ADAS13_4_mm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CDRSB', 'MMSE', 'RAVLT_immediate'], dtype='object')"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnk = abs(yrs_weight_mate.mean()).sort_values(ascending=False)\n",
    "xdf.columns[[x for x in range(len(jnk)) if jnk[x]>0.001]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grand_x = pandas.concat([xdf,x_years],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> working on iteration 0 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 29.0840850466 percent \n",
      " p = 0.0 \n",
      " r = 0.539296625676\n",
      "testing prediction accuracy is 60.1258142308 percent \n",
      " p = 0.0 \n",
      " r = 0.775408371317\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      "***** 17 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 1 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 32.4869702279 percent \n",
      " p = 0.0 \n",
      " r = 0.569973422432\n",
      "testing prediction accuracy is 74.9731904207 percent \n",
      " p = 0.0 \n",
      " r = 0.865870604771\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 2 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 32.5804764646 percent \n",
      " p = 0.0 \n",
      " r = 0.570793101435\n",
      "testing prediction accuracy is 72.4994854795 percent \n",
      " p = 0.0 \n",
      " r = 0.851466296922\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 3 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "4 features selected\n",
      "validation prediction accuracy is 31.5936997339 percent \n",
      " p = 0.0 \n",
      " r = 0.562082731757\n",
      "testing prediction accuracy is 70.9566863449 percent \n",
      " p = 0.0 \n",
      " r = 0.84235791885\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 4 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 31.0782098014 percent \n",
      " p = 0.0 \n",
      " r = 0.557478338605\n",
      "testing prediction accuracy is 71.3522583559 percent \n",
      " p = 3.13141535859e-302 \n",
      " r = 0.844702659851\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 5 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 32.3291624346 percent \n",
      " p = 0.0 \n",
      " r = 0.568587393763\n",
      "testing prediction accuracy is 85.0747510437 percent \n",
      " p = 0.0 \n",
      " r = 0.922359751093\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 6 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 33.620395015 percent \n",
      " p = 0.0 \n",
      " r = 0.579830966877\n",
      "testing prediction accuracy is 87.2147745772 percent \n",
      " p = 1.44411390998e-300 \n",
      " r = 0.933888508213\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 7 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 34.3849354764 percent \n",
      " p = 0.0 \n",
      " r = 0.586386693883\n",
      "testing prediction accuracy is 86.703094647 percent \n",
      " p = 1.1582347461e-203 \n",
      " r = 0.931144965336\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 8 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 35.2694469696 percent \n",
      " p = 0.0 \n",
      " r = 0.593880854799\n",
      "testing prediction accuracy is 86.6213551446 percent \n",
      " p = 9.40107926711e-167 \n",
      " r = 0.930705942522\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 9 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 34.7108821599 percent \n",
      " p = 0.0 \n",
      " r = 0.589159419511\n",
      "testing prediction accuracy is 89.2968623216 percent \n",
      " p = 1.83675391562e-135 \n",
      " r = 0.944970170543\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 10 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 34.8242222758 percent \n",
      " p = 0.0 \n",
      " r = 0.590120515453\n",
      "testing prediction accuracy is 91.9511383426 percent \n",
      " p = 2.09661899822e-134 \n",
      " r = 0.958911561838\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 11 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 34.3438608646 percent \n",
      " p = 0.0 \n",
      " r = 0.586036354372\n",
      "testing prediction accuracy is 90.7526044493 percent \n",
      " p = 3.39797961767e-115 \n",
      " r = 0.952641613879\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 12 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 34.1456944154 percent \n",
      " p = 0.0 \n",
      " r = 0.584343173276\n",
      "testing prediction accuracy is 89.3366606572 percent \n",
      " p = 7.90072532606e-93 \n",
      " r = 0.945180726936\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 13 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 36.0469261116 percent \n",
      " p = 0.0 \n",
      " r = 0.600390923579\n",
      "testing prediction accuracy is 90.4173672481 percent \n",
      " p = 2.18564412704e-83 \n",
      " r = 0.950880472237\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 14 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 36.1502914203 percent \n",
      " p = 0.0 \n",
      " r = 0.601251124076\n",
      "testing prediction accuracy is 88.4687988598 percent \n",
      " p = 1.34649927373e-58 \n",
      " r = 0.940578539304\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 15 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 33.160554273 percent \n",
      " p = 0.0 \n",
      " r = 0.57585201461\n",
      "testing prediction accuracy is 88.3794179237 percent \n",
      " p = 1.1333805886e-38 \n",
      " r = 0.940103281154\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 16 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 36.6210207688 percent \n",
      " p = 0.0 \n",
      " r = 0.605153044847\n",
      "testing prediction accuracy is 86.3335748708 percent \n",
      " p = 2.94551501739e-16 \n",
      " r = 0.929158624083\n",
      "***** 1 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 17 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 38.7731424241 percent \n",
      " p = 0.0 \n",
      " r = 0.622680836577\n",
      "testing prediction accuracy is 80.5953097649 percent \n",
      " p = 0.000422026777524 \n",
      " r = 0.89774890568\n",
      ">>>>>>>>>> working on iteration 18 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "5 features selected\n",
      "validation prediction accuracy is 35.5257427524 percent \n",
      " p = 0.0 \n",
      " r = 0.596034753621\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 748)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-200-8c200ef14bb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrand_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheld_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_years\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_ADAS13'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_feature_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0myrs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0myrs_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_same_tp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Science/scripts/kfold_learning.py\u001b[0m in \u001b[0;36mkfold_feature_learning\u001b[0;34m(train, test, y, t_y, clf, problem, folds, scale, verbose, search, p_cutoff, regcols, regdf, keep_cols, out_dir, output, save_int, vote, weighted)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mntest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 431\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 748)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "grand_results = pandas.DataFrame(np.zeros((20,1)),columns = ['n'])\n",
    "grand_weight_mate = pandas.DataFrame(np.zeros((20,grand_x.shape[-1])))\n",
    "for i in range(max(list(tps.values()))):\n",
    "    print('>'*10,'working on iteration %s of %s'%(i,max(list(tps.values()))),'<'*10)\n",
    "    results.ix[i,'n'] = len(np.array(list(tps.values()))[np.array(list(tps.values()))>(i+1)])\n",
    "    held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]\n",
    "    train = grand_x.drop(grand_x.index[held_idx], axis=0)\n",
    "    y = y_years.loc[train.index]['y_ADAS13']\n",
    "    test = grand_x.loc[held_idx]\n",
    "    t_y = y_years.loc[test.index]['y_ADAS13']\n",
    "    output = kfl.kfold_feature_learning(train, test, y, t_y, scale = False, weighted = True)\n",
    "    grand_results.ix[i,'validation'] = output[0][0]\n",
    "    grand_results.ix[i,'test_same_tp'] = output[0][1]\n",
    "    grand_weight_mate.loc[i] = output[1]\n",
    "    \n",
    "    tdf = y_years.loc[held_idx]\n",
    "    for ii in range(1,max(list(tps.values()))-(i+1)):\n",
    "        print('*'*5,ii,'timepoints ahead...','*'*5)\n",
    "        t_y = pandas.Series([y_years.loc[y_years[y_years.RID==x\n",
    "                                        ].index[i+ii]]['y_ADAS13'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "        test = pandas.DataFrame(grand_x.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]\n",
    "        ntest = check_array(test,accept_sparse='csr')\n",
    "        t_predicted = pandas.Series(safe_sparse_dot(ntest,np.array(output[1]).T,dense_output=True),index=test.index)\n",
    "        r,p = stats.pearsonr(t_y[test.index],t_predicted)\n",
    "        \n",
    "        grand_results.ix[i,'prediction_%s'%(ii)] = (r**2)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>validation</th>\n",
       "      <th>test_same_tp</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>prediction_6</th>\n",
       "      <th>prediction_7</th>\n",
       "      <th>prediction_8</th>\n",
       "      <th>prediction_9</th>\n",
       "      <th>prediction_10</th>\n",
       "      <th>prediction_11</th>\n",
       "      <th>prediction_12</th>\n",
       "      <th>prediction_13</th>\n",
       "      <th>prediction_14</th>\n",
       "      <th>prediction_15</th>\n",
       "      <th>prediction_16</th>\n",
       "      <th>prediction_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>29.084085</td>\n",
       "      <td>60.125814</td>\n",
       "      <td>52.787105</td>\n",
       "      <td>46.905772</td>\n",
       "      <td>30.839685</td>\n",
       "      <td>6.352402</td>\n",
       "      <td>1.467767</td>\n",
       "      <td>10.732035</td>\n",
       "      <td>9.404197</td>\n",
       "      <td>8.867628</td>\n",
       "      <td>8.576286</td>\n",
       "      <td>6.528809</td>\n",
       "      <td>5.915429</td>\n",
       "      <td>4.650797</td>\n",
       "      <td>5.065220</td>\n",
       "      <td>0.193246</td>\n",
       "      <td>15.568960</td>\n",
       "      <td>12.555583</td>\n",
       "      <td>37.801766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.486970</td>\n",
       "      <td>74.973190</td>\n",
       "      <td>61.754147</td>\n",
       "      <td>41.013675</td>\n",
       "      <td>7.885218</td>\n",
       "      <td>3.167232</td>\n",
       "      <td>18.493715</td>\n",
       "      <td>23.407562</td>\n",
       "      <td>22.866540</td>\n",
       "      <td>22.217852</td>\n",
       "      <td>25.928389</td>\n",
       "      <td>30.291636</td>\n",
       "      <td>27.496764</td>\n",
       "      <td>30.845557</td>\n",
       "      <td>24.105148</td>\n",
       "      <td>37.109161</td>\n",
       "      <td>34.646920</td>\n",
       "      <td>49.539223</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.580476</td>\n",
       "      <td>72.499485</td>\n",
       "      <td>42.517197</td>\n",
       "      <td>6.798131</td>\n",
       "      <td>3.207663</td>\n",
       "      <td>16.857064</td>\n",
       "      <td>16.901720</td>\n",
       "      <td>9.839697</td>\n",
       "      <td>5.104636</td>\n",
       "      <td>11.124816</td>\n",
       "      <td>11.771513</td>\n",
       "      <td>9.163162</td>\n",
       "      <td>12.924037</td>\n",
       "      <td>15.620909</td>\n",
       "      <td>32.083361</td>\n",
       "      <td>23.782250</td>\n",
       "      <td>18.997328</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.593700</td>\n",
       "      <td>70.956686</td>\n",
       "      <td>11.034153</td>\n",
       "      <td>1.149577</td>\n",
       "      <td>9.193903</td>\n",
       "      <td>9.940574</td>\n",
       "      <td>3.608966</td>\n",
       "      <td>0.712372</td>\n",
       "      <td>1.254072</td>\n",
       "      <td>0.845764</td>\n",
       "      <td>0.543358</td>\n",
       "      <td>0.758702</td>\n",
       "      <td>0.493428</td>\n",
       "      <td>0.660138</td>\n",
       "      <td>0.984143</td>\n",
       "      <td>0.867749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>31.078210</td>\n",
       "      <td>71.352258</td>\n",
       "      <td>4.029483</td>\n",
       "      <td>0.023080</td>\n",
       "      <td>0.059973</td>\n",
       "      <td>0.873413</td>\n",
       "      <td>2.534204</td>\n",
       "      <td>6.255359</td>\n",
       "      <td>9.823494</td>\n",
       "      <td>7.872422</td>\n",
       "      <td>5.456489</td>\n",
       "      <td>4.198331</td>\n",
       "      <td>9.065449</td>\n",
       "      <td>6.729974</td>\n",
       "      <td>2.635214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>32.329162</td>\n",
       "      <td>85.074751</td>\n",
       "      <td>22.063627</td>\n",
       "      <td>8.084369</td>\n",
       "      <td>12.598770</td>\n",
       "      <td>13.543877</td>\n",
       "      <td>30.874127</td>\n",
       "      <td>31.155854</td>\n",
       "      <td>31.906234</td>\n",
       "      <td>40.408083</td>\n",
       "      <td>33.795287</td>\n",
       "      <td>33.382461</td>\n",
       "      <td>28.316777</td>\n",
       "      <td>7.401693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.620395</td>\n",
       "      <td>87.214775</td>\n",
       "      <td>28.459186</td>\n",
       "      <td>9.766929</td>\n",
       "      <td>8.235185</td>\n",
       "      <td>17.049267</td>\n",
       "      <td>25.943285</td>\n",
       "      <td>28.031414</td>\n",
       "      <td>31.004192</td>\n",
       "      <td>27.813152</td>\n",
       "      <td>32.578157</td>\n",
       "      <td>21.631205</td>\n",
       "      <td>35.749515</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.384935</td>\n",
       "      <td>86.703095</td>\n",
       "      <td>46.877766</td>\n",
       "      <td>31.838138</td>\n",
       "      <td>38.635225</td>\n",
       "      <td>37.202524</td>\n",
       "      <td>41.437216</td>\n",
       "      <td>40.983406</td>\n",
       "      <td>34.878844</td>\n",
       "      <td>32.551042</td>\n",
       "      <td>27.072191</td>\n",
       "      <td>50.602972</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0</td>\n",
       "      <td>35.269447</td>\n",
       "      <td>86.621355</td>\n",
       "      <td>67.212419</td>\n",
       "      <td>56.240047</td>\n",
       "      <td>46.971169</td>\n",
       "      <td>46.486973</td>\n",
       "      <td>46.420079</td>\n",
       "      <td>34.609265</td>\n",
       "      <td>35.233632</td>\n",
       "      <td>30.969484</td>\n",
       "      <td>37.579814</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.710882</td>\n",
       "      <td>89.296862</td>\n",
       "      <td>79.326432</td>\n",
       "      <td>69.262544</td>\n",
       "      <td>64.893091</td>\n",
       "      <td>65.656089</td>\n",
       "      <td>50.166735</td>\n",
       "      <td>48.371557</td>\n",
       "      <td>38.383668</td>\n",
       "      <td>47.854490</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.824222</td>\n",
       "      <td>91.951138</td>\n",
       "      <td>84.515289</td>\n",
       "      <td>79.199555</td>\n",
       "      <td>77.841200</td>\n",
       "      <td>62.622170</td>\n",
       "      <td>58.241491</td>\n",
       "      <td>44.575435</td>\n",
       "      <td>65.809893</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.343861</td>\n",
       "      <td>90.752604</td>\n",
       "      <td>83.192881</td>\n",
       "      <td>83.466433</td>\n",
       "      <td>72.260686</td>\n",
       "      <td>65.483174</td>\n",
       "      <td>49.691402</td>\n",
       "      <td>71.662846</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.0</td>\n",
       "      <td>34.145694</td>\n",
       "      <td>89.336661</td>\n",
       "      <td>87.135617</td>\n",
       "      <td>78.672816</td>\n",
       "      <td>71.441547</td>\n",
       "      <td>53.497876</td>\n",
       "      <td>72.198014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.046926</td>\n",
       "      <td>90.417367</td>\n",
       "      <td>82.027095</td>\n",
       "      <td>74.773290</td>\n",
       "      <td>60.512374</td>\n",
       "      <td>76.925624</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.150291</td>\n",
       "      <td>88.468799</td>\n",
       "      <td>78.634647</td>\n",
       "      <td>60.789394</td>\n",
       "      <td>79.130578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.0</td>\n",
       "      <td>33.160554</td>\n",
       "      <td>88.379418</td>\n",
       "      <td>82.151355</td>\n",
       "      <td>80.257034</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.0</td>\n",
       "      <td>36.621021</td>\n",
       "      <td>86.333575</td>\n",
       "      <td>79.159594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.0</td>\n",
       "      <td>38.773142</td>\n",
       "      <td>80.595310</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      n  validation  test_same_tp  prediction_1  prediction_2  prediction_3  \\\n",
       "0   0.0   29.084085     60.125814     52.787105     46.905772     30.839685   \n",
       "1   0.0   32.486970     74.973190     61.754147     41.013675      7.885218   \n",
       "2   0.0   32.580476     72.499485     42.517197      6.798131      3.207663   \n",
       "3   0.0   31.593700     70.956686     11.034153      1.149577      9.193903   \n",
       "4   0.0   31.078210     71.352258      4.029483      0.023080      0.059973   \n",
       "5   0.0   32.329162     85.074751     22.063627      8.084369     12.598770   \n",
       "6   0.0   33.620395     87.214775     28.459186      9.766929      8.235185   \n",
       "7   0.0   34.384935     86.703095     46.877766     31.838138     38.635225   \n",
       "8   0.0   35.269447     86.621355     67.212419     56.240047     46.971169   \n",
       "9   0.0   34.710882     89.296862     79.326432     69.262544     64.893091   \n",
       "10  0.0   34.824222     91.951138     84.515289     79.199555     77.841200   \n",
       "11  0.0   34.343861     90.752604     83.192881     83.466433     72.260686   \n",
       "12  0.0   34.145694     89.336661     87.135617     78.672816     71.441547   \n",
       "13  0.0   36.046926     90.417367     82.027095     74.773290     60.512374   \n",
       "14  0.0   36.150291     88.468799     78.634647     60.789394     79.130578   \n",
       "15  0.0   33.160554     88.379418     82.151355     80.257034           NaN   \n",
       "16  0.0   36.621021     86.333575     79.159594           NaN           NaN   \n",
       "17  0.0   38.773142     80.595310           NaN           NaN           NaN   \n",
       "18  0.0         NaN           NaN           NaN           NaN           NaN   \n",
       "19  0.0         NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_4  prediction_5  prediction_6  prediction_7  prediction_8  \\\n",
       "0       6.352402      1.467767     10.732035      9.404197      8.867628   \n",
       "1       3.167232     18.493715     23.407562     22.866540     22.217852   \n",
       "2      16.857064     16.901720      9.839697      5.104636     11.124816   \n",
       "3       9.940574      3.608966      0.712372      1.254072      0.845764   \n",
       "4       0.873413      2.534204      6.255359      9.823494      7.872422   \n",
       "5      13.543877     30.874127     31.155854     31.906234     40.408083   \n",
       "6      17.049267     25.943285     28.031414     31.004192     27.813152   \n",
       "7      37.202524     41.437216     40.983406     34.878844     32.551042   \n",
       "8      46.486973     46.420079     34.609265     35.233632     30.969484   \n",
       "9      65.656089     50.166735     48.371557     38.383668     47.854490   \n",
       "10     62.622170     58.241491     44.575435     65.809893           NaN   \n",
       "11     65.483174     49.691402     71.662846           NaN           NaN   \n",
       "12     53.497876     72.198014           NaN           NaN           NaN   \n",
       "13     76.925624           NaN           NaN           NaN           NaN   \n",
       "14           NaN           NaN           NaN           NaN           NaN   \n",
       "15           NaN           NaN           NaN           NaN           NaN   \n",
       "16           NaN           NaN           NaN           NaN           NaN   \n",
       "17           NaN           NaN           NaN           NaN           NaN   \n",
       "18           NaN           NaN           NaN           NaN           NaN   \n",
       "19           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_9  prediction_10  prediction_11  prediction_12  prediction_13  \\\n",
       "0       8.576286       6.528809       5.915429       4.650797       5.065220   \n",
       "1      25.928389      30.291636      27.496764      30.845557      24.105148   \n",
       "2      11.771513       9.163162      12.924037      15.620909      32.083361   \n",
       "3       0.543358       0.758702       0.493428       0.660138       0.984143   \n",
       "4       5.456489       4.198331       9.065449       6.729974       2.635214   \n",
       "5      33.795287      33.382461      28.316777       7.401693            NaN   \n",
       "6      32.578157      21.631205      35.749515            NaN            NaN   \n",
       "7      27.072191      50.602972            NaN            NaN            NaN   \n",
       "8      37.579814            NaN            NaN            NaN            NaN   \n",
       "9            NaN            NaN            NaN            NaN            NaN   \n",
       "10           NaN            NaN            NaN            NaN            NaN   \n",
       "11           NaN            NaN            NaN            NaN            NaN   \n",
       "12           NaN            NaN            NaN            NaN            NaN   \n",
       "13           NaN            NaN            NaN            NaN            NaN   \n",
       "14           NaN            NaN            NaN            NaN            NaN   \n",
       "15           NaN            NaN            NaN            NaN            NaN   \n",
       "16           NaN            NaN            NaN            NaN            NaN   \n",
       "17           NaN            NaN            NaN            NaN            NaN   \n",
       "18           NaN            NaN            NaN            NaN            NaN   \n",
       "19           NaN            NaN            NaN            NaN            NaN   \n",
       "\n",
       "    prediction_14  prediction_15  prediction_16  prediction_17  \n",
       "0        0.193246      15.568960      12.555583      37.801766  \n",
       "1       37.109161      34.646920      49.539223            NaN  \n",
       "2       23.782250      18.997328            NaN            NaN  \n",
       "3        0.867749            NaN            NaN            NaN  \n",
       "4             NaN            NaN            NaN            NaN  \n",
       "5             NaN            NaN            NaN            NaN  \n",
       "6             NaN            NaN            NaN            NaN  \n",
       "7             NaN            NaN            NaN            NaN  \n",
       "8             NaN            NaN            NaN            NaN  \n",
       "9             NaN            NaN            NaN            NaN  \n",
       "10            NaN            NaN            NaN            NaN  \n",
       "11            NaN            NaN            NaN            NaN  \n",
       "12            NaN            NaN            NaN            NaN  \n",
       "13            NaN            NaN            NaN            NaN  \n",
       "14            NaN            NaN            NaN            NaN  \n",
       "15            NaN            NaN            NaN            NaN  \n",
       "16            NaN            NaN            NaN            NaN  \n",
       "17            NaN            NaN            NaN            NaN  \n",
       "18            NaN            NaN            NaN            NaN  \n",
       "19            NaN            NaN            NaN            NaN  "
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrs_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([], dtype='object')"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnk = abs(yrs_weight_mate.mean()).sort_values(ascending=False)\n",
    "xdf.columns[[x for x in range(len(jnk)) if jnk[x]>0.001]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      9.033123e-08\n",
       "127    0.000000e+00\n",
       "118    0.000000e+00\n",
       "119    0.000000e+00\n",
       "120    0.000000e+00\n",
       "121    0.000000e+00\n",
       "122    0.000000e+00\n",
       "123    0.000000e+00\n",
       "124    0.000000e+00\n",
       "125    0.000000e+00\n",
       "126    0.000000e+00\n",
       "128    0.000000e+00\n",
       "139    0.000000e+00\n",
       "129    0.000000e+00\n",
       "130    0.000000e+00\n",
       "131    0.000000e+00\n",
       "132    0.000000e+00\n",
       "133    0.000000e+00\n",
       "134    0.000000e+00\n",
       "135    0.000000e+00\n",
       "136    0.000000e+00\n",
       "137    0.000000e+00\n",
       "117    0.000000e+00\n",
       "116    0.000000e+00\n",
       "115    0.000000e+00\n",
       "114    0.000000e+00\n",
       "95     0.000000e+00\n",
       "96     0.000000e+00\n",
       "97     0.000000e+00\n",
       "98     0.000000e+00\n",
       "           ...     \n",
       "272    0.000000e+00\n",
       "273    0.000000e+00\n",
       "274    0.000000e+00\n",
       "275    0.000000e+00\n",
       "276    0.000000e+00\n",
       "277    0.000000e+00\n",
       "258    0.000000e+00\n",
       "256    0.000000e+00\n",
       "235    0.000000e+00\n",
       "255    0.000000e+00\n",
       "236    0.000000e+00\n",
       "237    0.000000e+00\n",
       "238    0.000000e+00\n",
       "239    0.000000e+00\n",
       "240    0.000000e+00\n",
       "241    0.000000e+00\n",
       "242    0.000000e+00\n",
       "243    0.000000e+00\n",
       "244    0.000000e+00\n",
       "245    0.000000e+00\n",
       "246    0.000000e+00\n",
       "247    0.000000e+00\n",
       "248    0.000000e+00\n",
       "249    0.000000e+00\n",
       "250    0.000000e+00\n",
       "251    0.000000e+00\n",
       "252    0.000000e+00\n",
       "253    0.000000e+00\n",
       "254    0.000000e+00\n",
       "186    0.000000e+00\n",
       "Length: 374, dtype: float64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yrs_weight_mate.mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>>>>>>>>> working on iteration 0 of 19 <<<<<<<<<<\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/users/jvogel/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "209 features selected\n",
      "validation prediction accuracy is 84.7322028088 percent \n",
      " p = 0.0 \n",
      " r = 0.920500965827\n",
      "testing prediction accuracy is 89.9678084964 percent \n",
      " p = 0.0 \n",
      " r = 0.948513618755\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      "***** 17 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 1 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "52 features selected\n",
      "validation prediction accuracy is 85.8276683856 percent \n",
      " p = 0.0 \n",
      " r = 0.926432233817\n",
      "testing prediction accuracy is 79.0557464292 percent \n",
      " p = 0.0 \n",
      " r = 0.889132984593\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      "***** 16 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 2 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "39 features selected\n",
      "validation prediction accuracy is 86.0878944873 percent \n",
      " p = 0.0 \n",
      " r = 0.927835623843\n",
      "testing prediction accuracy is 75.5409219201 percent \n",
      " p = 0.0 \n",
      " r = 0.869142807139\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      "***** 15 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 3 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "36 features selected\n",
      "validation prediction accuracy is 83.4660086965 percent \n",
      " p = 0.0 \n",
      " r = 0.913597333055\n",
      "testing prediction accuracy is 88.5194928672 percent \n",
      " p = 0.0 \n",
      " r = 0.940847983827\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      "***** 14 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 4 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "17 features selected\n",
      "validation prediction accuracy is 83.3723939566 percent \n",
      " p = 0.0 \n",
      " r = 0.913084847956\n",
      "testing prediction accuracy is 87.5496545852 percent \n",
      " p = 0.0 \n",
      " r = 0.935679723972\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      "***** 13 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 5 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "14 features selected\n",
      "validation prediction accuracy is 83.1348497935 percent \n",
      " p = 0.0 \n",
      " r = 0.911783141945\n",
      "testing prediction accuracy is 87.214991243 percent \n",
      " p = 0.0 \n",
      " r = 0.933889668232\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      "***** 12 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 6 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "14 features selected\n",
      "validation prediction accuracy is 83.4708732849 percent \n",
      " p = 0.0 \n",
      " r = 0.91362395593\n",
      "testing prediction accuracy is 85.8997803543 percent \n",
      " p = 2.31062866443e-286 \n",
      " r = 0.926821343919\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      "***** 11 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 7 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "23 features selected\n",
      "validation prediction accuracy is 83.824685841 percent \n",
      " p = 0.0 \n",
      " r = 0.915558222294\n",
      "testing prediction accuracy is 88.3368149975 percent \n",
      " p = 9.22737259961e-217 \n",
      " r = 0.939876667428\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      "***** 10 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 8 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "28 features selected\n",
      "validation prediction accuracy is 84.0516450056 percent \n",
      " p = 0.0 \n",
      " r = 0.916796842302\n",
      "testing prediction accuracy is 88.9393824063 percent \n",
      " p = 2.46152694243e-182 \n",
      " r = 0.943076785879\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      "***** 9 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 9 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "27 features selected\n",
      "validation prediction accuracy is 84.0200937813 percent \n",
      " p = 0.0 \n",
      " r = 0.916624753\n",
      "testing prediction accuracy is 90.5264212552 percent \n",
      " p = 9.41915273149e-143 \n",
      " r = 0.951453736422\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      "***** 8 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 10 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "31 features selected\n",
      "validation prediction accuracy is 84.1133287368 percent \n",
      " p = 0.0 \n",
      " r = 0.917133189547\n",
      "testing prediction accuracy is 89.6996653651 percent \n",
      " p = 1.94086623233e-121 \n",
      " r = 0.947099072775\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      "***** 7 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 11 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "30 features selected\n",
      "validation prediction accuracy is 84.265873482 percent \n",
      " p = 0.0 \n",
      " r = 0.917964451828\n",
      "testing prediction accuracy is 86.9842974683 percent \n",
      " p = 6.24326871717e-99 \n",
      " r = 0.932653727105\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      "***** 6 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 12 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "29 features selected\n",
      "validation prediction accuracy is 84.3268087517 percent \n",
      " p = 0.0 \n",
      " r = 0.918296296147\n",
      "testing prediction accuracy is 87.018475315 percent \n",
      " p = 7.78473916213e-85 \n",
      " r = 0.932836938136\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      "***** 5 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 13 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "31 features selected\n",
      "validation prediction accuracy is 84.3505336051 percent \n",
      " p = 0.0 \n",
      " r = 0.918425465703\n",
      "testing prediction accuracy is 87.4316024976 percent \n",
      " p = 5.89352805914e-74 \n",
      " r = 0.935048675191\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      "***** 4 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 14 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "33 features selected\n",
      "validation prediction accuracy is 84.3869077191 percent \n",
      " p = 0.0 \n",
      " r = 0.91862346867\n",
      "testing prediction accuracy is 83.4582486898 percent \n",
      " p = 4.19177872961e-49 \n",
      " r = 0.913554862555\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      "***** 3 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 15 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "35 features selected\n",
      "validation prediction accuracy is 84.4163199589 percent \n",
      " p = 0.0 \n",
      " r = 0.918783543382\n",
      "testing prediction accuracy is 87.8994319658 percent \n",
      " p = 5.62149095421e-38 \n",
      " r = 0.937546969307\n",
      "***** 1 timepoints ahead... *****\n",
      "***** 2 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 16 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "33 features selected\n",
      "validation prediction accuracy is 84.3536384405 percent \n",
      " p = 0.0 \n",
      " r = 0.918442368581\n",
      "testing prediction accuracy is 82.2540756662 percent \n",
      " p = 2.55586876579e-14 \n",
      " r = 0.906940326957\n",
      "***** 1 timepoints ahead... *****\n",
      ">>>>>>>>>> working on iteration 17 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "30 features selected\n",
      "validation prediction accuracy is 84.2768237284 percent \n",
      " p = 0.0 \n",
      " r = 0.918024094065\n",
      "testing prediction accuracy is 71.0580984197 percent \n",
      " p = 0.00219150458255 \n",
      " r = 0.842959657515\n",
      ">>>>>>>>>> working on iteration 18 of 19 <<<<<<<<<<\n",
      "running model for fold 1 of 10\n",
      "running model for fold 2 of 10\n",
      "running model for fold 3 of 10\n",
      "running model for fold 4 of 10\n",
      "running model for fold 5 of 10\n",
      "running model for fold 6 of 10\n",
      "running model for fold 7 of 10\n",
      "running model for fold 8 of 10\n",
      "running model for fold 9 of 10\n",
      "running model for fold 10 of 10\n",
      "30 features selected\n",
      "validation prediction accuracy is 84.2422969988 percent \n",
      " p = 0.0 \n",
      " r = 0.917836025654\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 374)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-220-2eccb98925e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mheld_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mt_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mydf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ventr_ICV_ratio'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkfl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkfold_feature_learning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mv_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'validation'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mv_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'test_same_tp'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Science/scripts/kfold_learning.py\u001b[0m in \u001b[0;36mkfold_feature_learning\u001b[0;34m(train, test, y, t_y, clf, problem, folds, scale, verbose, search, p_cutoff, regcols, regdf, keep_cols, out_dir, output, save_int, vote, weighted)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvote_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_mods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproblem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvote\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0mntest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mt_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msave_int\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    429\u001b[0m                              \u001b[0;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[0;32m--> 431\u001b[0;31m                                 context))\n\u001b[0m\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 374)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "v_results = pandas.DataFrame(np.zeros((20,1)),columns = ['n'])\n",
    "v_weight_mate = pandas.DataFrame(np.zeros((20,xdf.shape[-1])))\n",
    "for i in range(max(list(tps.values()))):\n",
    "    print('>'*10,'working on iteration %s of %s'%(i,max(list(tps.values()))),'<'*10)\n",
    "    v_results.ix[i,'n'] = len(np.array(list(tps.values()))[np.array(list(tps.values()))>(i+1)])\n",
    "    held_idx = [ydf[ydf.RID==x].index[i] for x in ydf.RID.unique() if tps[x]>(i+1)]\n",
    "    train = xdf.drop(xdf.index[held_idx], axis=0)\n",
    "    y = ydf.loc[train.index]['ventr_ICV_ratio']\n",
    "    test = xdf.loc[held_idx]\n",
    "    t_y = ydf.loc[test.index]['ventr_ICV_ratio']\n",
    "    output = kfl.kfold_feature_learning(train, test, y, t_y, scale = False, weighted = True)\n",
    "    v_results.ix[i,'validation'] = output[0][0]\n",
    "    v_results.ix[i,'test_same_tp'] = output[0][1]\n",
    "    v_weight_mate.loc[i] = output[1]\n",
    "    \n",
    "    tdf = ydf.loc[held_idx]\n",
    "    for ii in range(1,max(list(tps.values()))-(i+1)):\n",
    "        print('*'*5,ii,'timepoints ahead...','*'*5)\n",
    "        t_y = pandas.Series([ydf.loc[ydf[ydf.RID==x\n",
    "                                        ].index[i+ii]]['ventr_ICV_ratio'] if tps[x]>(i+ii) else np.nan for x in tdf.RID.unique()\n",
    "                               ]).dropna()\n",
    "        test = pandas.DataFrame(xdf.loc[held_idx].reset_index(drop=True), copy = True).loc[t_y.index]\n",
    "        ntest = check_array(test,accept_sparse='csr')\n",
    "        t_predicted = pandas.Series(safe_sparse_dot(ntest,np.array(output[1]).T,dense_output=True),index=test.index)\n",
    "        r,p = stats.pearsonr(t_y[test.index],t_predicted)\n",
    "        \n",
    "        v_results.ix[i,'prediction_%s'%(ii)] = (r**2)*100\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['AGE', 'PTRACCAT', 'Hippocampus', 'WholeBrain', 'ICV', 'Ventricles_bl',\n",
       "       'ICV_bl', 'VENTQC_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST10CV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST120SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST127SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST130TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST21SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST29SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST35TA_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST37SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST54TS_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST5SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST61SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST6SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST80SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16',\n",
       "       'ST96SV_UCSFFSX_11_02_15_UCSFFSX51_08_01_16'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jnk = abs(v_weight_mate.mean()).sort_values(ascending=False)\n",
    "xdf.columns[[x for x in range(len(jnk)) if jnk[x]>0.001]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>validation</th>\n",
       "      <th>test_same_tp</th>\n",
       "      <th>prediction_1</th>\n",
       "      <th>prediction_2</th>\n",
       "      <th>prediction_3</th>\n",
       "      <th>prediction_4</th>\n",
       "      <th>prediction_5</th>\n",
       "      <th>prediction_6</th>\n",
       "      <th>prediction_7</th>\n",
       "      <th>prediction_8</th>\n",
       "      <th>prediction_9</th>\n",
       "      <th>prediction_10</th>\n",
       "      <th>prediction_11</th>\n",
       "      <th>prediction_12</th>\n",
       "      <th>prediction_13</th>\n",
       "      <th>prediction_14</th>\n",
       "      <th>prediction_15</th>\n",
       "      <th>prediction_16</th>\n",
       "      <th>prediction_17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1667.0</td>\n",
       "      <td>84.732203</td>\n",
       "      <td>89.967808</td>\n",
       "      <td>79.032231</td>\n",
       "      <td>75.824989</td>\n",
       "      <td>86.749546</td>\n",
       "      <td>86.410480</td>\n",
       "      <td>84.381785</td>\n",
       "      <td>82.995539</td>\n",
       "      <td>82.710855</td>\n",
       "      <td>82.116661</td>\n",
       "      <td>79.566893</td>\n",
       "      <td>79.001960</td>\n",
       "      <td>75.470052</td>\n",
       "      <td>72.956446</td>\n",
       "      <td>71.815451</td>\n",
       "      <td>63.452055</td>\n",
       "      <td>58.376495</td>\n",
       "      <td>52.611451</td>\n",
       "      <td>50.187952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1590.0</td>\n",
       "      <td>85.827668</td>\n",
       "      <td>79.055746</td>\n",
       "      <td>75.187125</td>\n",
       "      <td>87.012708</td>\n",
       "      <td>86.055098</td>\n",
       "      <td>84.573320</td>\n",
       "      <td>82.847612</td>\n",
       "      <td>82.811810</td>\n",
       "      <td>82.702731</td>\n",
       "      <td>79.863311</td>\n",
       "      <td>80.532428</td>\n",
       "      <td>76.586525</td>\n",
       "      <td>73.536602</td>\n",
       "      <td>72.048598</td>\n",
       "      <td>63.077445</td>\n",
       "      <td>57.960202</td>\n",
       "      <td>52.096603</td>\n",
       "      <td>46.603264</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1496.0</td>\n",
       "      <td>86.087894</td>\n",
       "      <td>75.540922</td>\n",
       "      <td>86.794993</td>\n",
       "      <td>86.035573</td>\n",
       "      <td>84.323708</td>\n",
       "      <td>83.236321</td>\n",
       "      <td>83.692743</td>\n",
       "      <td>84.309614</td>\n",
       "      <td>81.657564</td>\n",
       "      <td>82.863660</td>\n",
       "      <td>79.118219</td>\n",
       "      <td>76.054586</td>\n",
       "      <td>74.394349</td>\n",
       "      <td>64.273718</td>\n",
       "      <td>58.779631</td>\n",
       "      <td>51.600227</td>\n",
       "      <td>46.434990</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1348.0</td>\n",
       "      <td>83.466009</td>\n",
       "      <td>88.519493</td>\n",
       "      <td>86.093270</td>\n",
       "      <td>84.722186</td>\n",
       "      <td>83.302546</td>\n",
       "      <td>83.548389</td>\n",
       "      <td>83.436428</td>\n",
       "      <td>80.537744</td>\n",
       "      <td>82.100790</td>\n",
       "      <td>78.127414</td>\n",
       "      <td>74.883356</td>\n",
       "      <td>73.756642</td>\n",
       "      <td>63.811512</td>\n",
       "      <td>59.388080</td>\n",
       "      <td>52.586250</td>\n",
       "      <td>47.888629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1107.0</td>\n",
       "      <td>83.372394</td>\n",
       "      <td>87.549655</td>\n",
       "      <td>84.882530</td>\n",
       "      <td>82.490186</td>\n",
       "      <td>83.154217</td>\n",
       "      <td>82.531532</td>\n",
       "      <td>80.617427</td>\n",
       "      <td>82.140784</td>\n",
       "      <td>78.090091</td>\n",
       "      <td>74.416493</td>\n",
       "      <td>74.020275</td>\n",
       "      <td>64.828871</td>\n",
       "      <td>58.901782</td>\n",
       "      <td>51.625902</td>\n",
       "      <td>47.241204</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>942.0</td>\n",
       "      <td>83.134850</td>\n",
       "      <td>87.214991</td>\n",
       "      <td>82.779821</td>\n",
       "      <td>82.263924</td>\n",
       "      <td>80.802973</td>\n",
       "      <td>79.213867</td>\n",
       "      <td>81.259914</td>\n",
       "      <td>77.246669</td>\n",
       "      <td>74.422998</td>\n",
       "      <td>73.315122</td>\n",
       "      <td>66.140467</td>\n",
       "      <td>63.691870</td>\n",
       "      <td>58.146582</td>\n",
       "      <td>52.700115</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>670.0</td>\n",
       "      <td>83.470873</td>\n",
       "      <td>85.899780</td>\n",
       "      <td>83.656243</td>\n",
       "      <td>82.184472</td>\n",
       "      <td>80.359251</td>\n",
       "      <td>81.288775</td>\n",
       "      <td>77.715298</td>\n",
       "      <td>76.461682</td>\n",
       "      <td>73.923388</td>\n",
       "      <td>66.609485</td>\n",
       "      <td>62.922655</td>\n",
       "      <td>56.033874</td>\n",
       "      <td>52.513425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>462.0</td>\n",
       "      <td>83.824686</td>\n",
       "      <td>88.336815</td>\n",
       "      <td>83.684418</td>\n",
       "      <td>81.321355</td>\n",
       "      <td>81.693008</td>\n",
       "      <td>79.264093</td>\n",
       "      <td>77.250985</td>\n",
       "      <td>75.017662</td>\n",
       "      <td>68.445990</td>\n",
       "      <td>64.885565</td>\n",
       "      <td>57.413983</td>\n",
       "      <td>55.566672</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>379.0</td>\n",
       "      <td>84.051645</td>\n",
       "      <td>88.939382</td>\n",
       "      <td>84.484771</td>\n",
       "      <td>85.784269</td>\n",
       "      <td>80.912044</td>\n",
       "      <td>77.911807</td>\n",
       "      <td>75.017808</td>\n",
       "      <td>66.732930</td>\n",
       "      <td>62.477175</td>\n",
       "      <td>57.353163</td>\n",
       "      <td>49.869485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>277.0</td>\n",
       "      <td>84.020094</td>\n",
       "      <td>90.526421</td>\n",
       "      <td>87.474459</td>\n",
       "      <td>83.686040</td>\n",
       "      <td>80.078967</td>\n",
       "      <td>78.518662</td>\n",
       "      <td>70.056989</td>\n",
       "      <td>64.165014</td>\n",
       "      <td>57.896520</td>\n",
       "      <td>53.704165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>244.0</td>\n",
       "      <td>84.113329</td>\n",
       "      <td>89.699665</td>\n",
       "      <td>83.383582</td>\n",
       "      <td>81.559219</td>\n",
       "      <td>79.538940</td>\n",
       "      <td>71.265703</td>\n",
       "      <td>65.503993</td>\n",
       "      <td>58.515353</td>\n",
       "      <td>56.353275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>221.0</td>\n",
       "      <td>84.265873</td>\n",
       "      <td>86.984297</td>\n",
       "      <td>81.803143</td>\n",
       "      <td>82.054660</td>\n",
       "      <td>73.115868</td>\n",
       "      <td>68.272896</td>\n",
       "      <td>62.593602</td>\n",
       "      <td>59.035015</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>189.0</td>\n",
       "      <td>84.326809</td>\n",
       "      <td>87.018475</td>\n",
       "      <td>83.437290</td>\n",
       "      <td>75.628332</td>\n",
       "      <td>70.319230</td>\n",
       "      <td>64.445233</td>\n",
       "      <td>60.889977</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>162.0</td>\n",
       "      <td>84.350534</td>\n",
       "      <td>87.431602</td>\n",
       "      <td>75.701589</td>\n",
       "      <td>73.842659</td>\n",
       "      <td>66.714666</td>\n",
       "      <td>65.415684</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>123.0</td>\n",
       "      <td>84.386908</td>\n",
       "      <td>83.458249</td>\n",
       "      <td>73.051019</td>\n",
       "      <td>66.834323</td>\n",
       "      <td>65.967126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>81.0</td>\n",
       "      <td>84.416320</td>\n",
       "      <td>87.899432</td>\n",
       "      <td>81.928890</td>\n",
       "      <td>78.072671</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>36.0</td>\n",
       "      <td>84.353638</td>\n",
       "      <td>82.254076</td>\n",
       "      <td>74.605163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>10.0</td>\n",
       "      <td>84.276824</td>\n",
       "      <td>71.058098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         n  validation  test_same_tp  prediction_1  prediction_2  \\\n",
       "0   1667.0   84.732203     89.967808     79.032231     75.824989   \n",
       "1   1590.0   85.827668     79.055746     75.187125     87.012708   \n",
       "2   1496.0   86.087894     75.540922     86.794993     86.035573   \n",
       "3   1348.0   83.466009     88.519493     86.093270     84.722186   \n",
       "4   1107.0   83.372394     87.549655     84.882530     82.490186   \n",
       "5    942.0   83.134850     87.214991     82.779821     82.263924   \n",
       "6    670.0   83.470873     85.899780     83.656243     82.184472   \n",
       "7    462.0   83.824686     88.336815     83.684418     81.321355   \n",
       "8    379.0   84.051645     88.939382     84.484771     85.784269   \n",
       "9    277.0   84.020094     90.526421     87.474459     83.686040   \n",
       "10   244.0   84.113329     89.699665     83.383582     81.559219   \n",
       "11   221.0   84.265873     86.984297     81.803143     82.054660   \n",
       "12   189.0   84.326809     87.018475     83.437290     75.628332   \n",
       "13   162.0   84.350534     87.431602     75.701589     73.842659   \n",
       "14   123.0   84.386908     83.458249     73.051019     66.834323   \n",
       "15    81.0   84.416320     87.899432     81.928890     78.072671   \n",
       "16    36.0   84.353638     82.254076     74.605163           NaN   \n",
       "17    10.0   84.276824     71.058098           NaN           NaN   \n",
       "18     0.0         NaN           NaN           NaN           NaN   \n",
       "19     0.0         NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_3  prediction_4  prediction_5  prediction_6  prediction_7  \\\n",
       "0      86.749546     86.410480     84.381785     82.995539     82.710855   \n",
       "1      86.055098     84.573320     82.847612     82.811810     82.702731   \n",
       "2      84.323708     83.236321     83.692743     84.309614     81.657564   \n",
       "3      83.302546     83.548389     83.436428     80.537744     82.100790   \n",
       "4      83.154217     82.531532     80.617427     82.140784     78.090091   \n",
       "5      80.802973     79.213867     81.259914     77.246669     74.422998   \n",
       "6      80.359251     81.288775     77.715298     76.461682     73.923388   \n",
       "7      81.693008     79.264093     77.250985     75.017662     68.445990   \n",
       "8      80.912044     77.911807     75.017808     66.732930     62.477175   \n",
       "9      80.078967     78.518662     70.056989     64.165014     57.896520   \n",
       "10     79.538940     71.265703     65.503993     58.515353     56.353275   \n",
       "11     73.115868     68.272896     62.593602     59.035015           NaN   \n",
       "12     70.319230     64.445233     60.889977           NaN           NaN   \n",
       "13     66.714666     65.415684           NaN           NaN           NaN   \n",
       "14     65.967126           NaN           NaN           NaN           NaN   \n",
       "15           NaN           NaN           NaN           NaN           NaN   \n",
       "16           NaN           NaN           NaN           NaN           NaN   \n",
       "17           NaN           NaN           NaN           NaN           NaN   \n",
       "18           NaN           NaN           NaN           NaN           NaN   \n",
       "19           NaN           NaN           NaN           NaN           NaN   \n",
       "\n",
       "    prediction_8  prediction_9  prediction_10  prediction_11  prediction_12  \\\n",
       "0      82.116661     79.566893      79.001960      75.470052      72.956446   \n",
       "1      79.863311     80.532428      76.586525      73.536602      72.048598   \n",
       "2      82.863660     79.118219      76.054586      74.394349      64.273718   \n",
       "3      78.127414     74.883356      73.756642      63.811512      59.388080   \n",
       "4      74.416493     74.020275      64.828871      58.901782      51.625902   \n",
       "5      73.315122     66.140467      63.691870      58.146582      52.700115   \n",
       "6      66.609485     62.922655      56.033874      52.513425            NaN   \n",
       "7      64.885565     57.413983      55.566672            NaN            NaN   \n",
       "8      57.353163     49.869485            NaN            NaN            NaN   \n",
       "9      53.704165           NaN            NaN            NaN            NaN   \n",
       "10           NaN           NaN            NaN            NaN            NaN   \n",
       "11           NaN           NaN            NaN            NaN            NaN   \n",
       "12           NaN           NaN            NaN            NaN            NaN   \n",
       "13           NaN           NaN            NaN            NaN            NaN   \n",
       "14           NaN           NaN            NaN            NaN            NaN   \n",
       "15           NaN           NaN            NaN            NaN            NaN   \n",
       "16           NaN           NaN            NaN            NaN            NaN   \n",
       "17           NaN           NaN            NaN            NaN            NaN   \n",
       "18           NaN           NaN            NaN            NaN            NaN   \n",
       "19           NaN           NaN            NaN            NaN            NaN   \n",
       "\n",
       "    prediction_13  prediction_14  prediction_15  prediction_16  prediction_17  \n",
       "0       71.815451      63.452055      58.376495      52.611451      50.187952  \n",
       "1       63.077445      57.960202      52.096603      46.603264            NaN  \n",
       "2       58.779631      51.600227      46.434990            NaN            NaN  \n",
       "3       52.586250      47.888629            NaN            NaN            NaN  \n",
       "4       47.241204            NaN            NaN            NaN            NaN  \n",
       "5             NaN            NaN            NaN            NaN            NaN  \n",
       "6             NaN            NaN            NaN            NaN            NaN  \n",
       "7             NaN            NaN            NaN            NaN            NaN  \n",
       "8             NaN            NaN            NaN            NaN            NaN  \n",
       "9             NaN            NaN            NaN            NaN            NaN  \n",
       "10            NaN            NaN            NaN            NaN            NaN  \n",
       "11            NaN            NaN            NaN            NaN            NaN  \n",
       "12            NaN            NaN            NaN            NaN            NaN  \n",
       "13            NaN            NaN            NaN            NaN            NaN  \n",
       "14            NaN            NaN            NaN            NaN            NaN  \n",
       "15            NaN            NaN            NaN            NaN            NaN  \n",
       "16            NaN            NaN            NaN            NaN            NaN  \n",
       "17            NaN            NaN            NaN            NaN            NaN  \n",
       "18            NaN            NaN            NaN            NaN            NaN  \n",
       "19            NaN            NaN            NaN            NaN            NaN  "
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_results"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
