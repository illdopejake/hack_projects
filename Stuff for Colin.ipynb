{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import nibabel as ni\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas\n",
    "import numpy as np\n",
    "import nibabel as ni\n",
    "from glob import glob\n",
    "\n",
    "def main(image_dir, beta_dir, spreadsheet, outdir, sdres_img = '', beta_str = 'beta',\n",
    "                           intercept='first', res_str = 'Res_', \n",
    "                          cols_to_use = [], img_mask = None, subject_col = '',\n",
    "                          memory_load = 'low'):\n",
    "    '''This script will create W-SCORE images from a set of input images, and a\n",
    "    spreadsheet. This script makes several assumptions about the inputs:\n",
    "    \n",
    "    1) The \"image_dir\" path must ONLY contain input images, and no other files or \n",
    "        directories\n",
    "    2) The rows of your spreadsheet MUST be in the exact same order as the images \n",
    "        in the \"image_dir directory. So subject 1 should be row 1 of the spreadsheet \n",
    "        (not including headers)\n",
    "    3) The script assumes you have run an SPM model for the variables included in \n",
    "        the W-SCORE, and that you thus have BETA images for each of those variables\n",
    "    4) The number of BETA images (not including the intercept) should be in the exact \n",
    "        same order as either the columns of your spreadsheet, or the columns in the\n",
    "        cols_to_use argument.\n",
    "    5) If cols_to_use is not passed, script assumes that spreadsheet only includes \n",
    "        columns for which BETA images exist, as well as a subject column (but only\n",
    "        if the subject_col argument is passed)\n",
    "    6) The script assumes you either have an image representing the Standard\n",
    "        Deviation of the Residuals, or you have asked SPM to create the residuals in \n",
    "        the model described in 3)\n",
    "    \n",
    "    If all of these assumptions have been met, you may proceed.\n",
    "    \n",
    "    image_dir = an directory containing ONLY raw images to be W-transformed (REQUIRED)\n",
    "    beta_dir = a directory containing beta images generated from an SPM model (REQUIRED)\n",
    "    spreadsheet = a spreadsheet with one row for each subject in image_dir (same order),\n",
    "        and columns indicating values for variables to be accounted for (e.g. age, sex),\n",
    "        which correspond to, and are in the same order as, beta maps in beta_dir. (REQUIRED)\n",
    "    outdir = a directory to store the transformed output files (does not have to exist) (REQUIRED)\n",
    "    sdres_img = a path to an image representing the Standard Deviation of the Residuals (of the \n",
    "        normative model). Or, leave blank and script will create one for you using residual\n",
    "        images created by the SPM model (Res_str must be passed)\n",
    "    beta_str = string labeling beta images from the SPM model (the text in front of the image \n",
    "        names)\n",
    "    intercept = SPM puts the intercept as the first or last image, depending on the version.\n",
    "        Enter \"first\" if its first, or \"last\" if its last. If you did not include the\n",
    "        intercept in the model, pass \"none\". (WARNING, \"none\" is in beta and will crash)\n",
    "    res_str = if you did not pass an option for sdres_img, you must an argument here. This\n",
    "        is the string labels of Residual images created by the SPM model.\n",
    "    cols_to_use = a list of columns from df to use can be specified here. If your df has \n",
    "        columns you do not wish to use in the w-scoring, you can specify the names of the\n",
    "        columns you do wish to use (be sure they are in the exact same order as the beta\n",
    "        images in the beta_dir)\n",
    "    img_mask = path to a binarized mask image in the same dimensions as your input images. If\n",
    "        passed, the w-score images will be masked (RECOMMENDED)\n",
    "    subject_col = name of a column in df with the subject IDs. This will automatically label\n",
    "        the w-score images with the subject IDs from this columns\n",
    "    memory_load = Only relevant if sdres_img is not passed. If you are using a computer with\n",
    "        a lot of memory, change this to \"high\" to speed up the creation of the sdres_img. \n",
    "        However, this may crash your computer if you do not have enough memory storage. If\n",
    "        you're unsure, leave it as low.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # check inputs\n",
    "    print('initating and checking inputs')\n",
    "    for cdir in [image_dir, beta_dir]:\n",
    "        if type(cdir) != str:\n",
    "            raise TypeError('%s must be a path pointing to a valid directory, you passed a %s object'%(cdir,\n",
    "                                                                                                    type(cdir)))\n",
    "        if not os.path.isdir(image_dir):\n",
    "            raise IOError('could not find the directory specified in argument \"image_dir\"')\n",
    "    if type(outdir) != str:\n",
    "        raise TypeError('%s must be a path pointing to a valid directory, you passed a %s object'%(outdir,\n",
    "                                                                                                type(cdir)))\n",
    "    if not os.path.isdir(outdir):\n",
    "        os.mkdir(outdir)\n",
    "    \n",
    "    if intercept not in ['first','last', 'none']:\n",
    "        raise IOError('intercept must be set to \"first\", \"last\" or \"none\". See docstring for more info')\n",
    "    \n",
    "    # load inputs\n",
    "    raw_paths = glob(os.path.join(image_dir,'*'))\n",
    "    print('found %s images to transform'%len(raw_paths))\n",
    "    \n",
    "    beta_paths = glob(os.path.join(beta_dir,'%s*'%beta_str))\n",
    "    if len(beta_paths) == 0:\n",
    "        raise IOError('No beta images found in specified directory. Please revise beta_dir or beta_str arguments')\n",
    "    if intercept == 'none':\n",
    "        n_betas = len(beta_paths)\n",
    "    else:\n",
    "        n_betas = len(beta_paths) - 1\n",
    "    if len(cols_to_use) > 0:\n",
    "        if len(cols_to_use) != n_betas:\n",
    "            raise IOError('# of columns in cols_to_use (%s) must match number of non-intercept beta maps (%s)'%(\n",
    "                                                                                            len(cols_to_use),\n",
    "                                                                                            n_betas))\n",
    "\n",
    "    print('preparing spreadsheet')\n",
    "    if type(spreadsheet) = str\n",
    "        if '.csv' in spreadsheet:\n",
    "            df = pandas.read_csv(spreadsheet)\n",
    "        elif 'xl' in spreadsheet[-4:]:\n",
    "            try:\n",
    "                jnk = pandas.ExcelFile(spreadsheet)\n",
    "                df = pandas.ExcelFile(jnk).parse(jnk.sheet_names[0])\n",
    "            except:\n",
    "                raise IOError('could not load excel file. Please convert to csv')\n",
    "        else:\n",
    "            try:\n",
    "                df = pandas.read_table(spreadsheet)\n",
    "            except:\n",
    "                raise IOError('could not read spreadsheet. Try loading the df yourself with pandas and inputting that')\n",
    "    else:\n",
    "        df = spreadsheet\n",
    "    if subject_col:\n",
    "        if type(subject_col) == int:\n",
    "            subject_col = df.columns[subject_col]\n",
    "            subject_IDs = df[subject_col].values\n",
    "            print('using %s for subject IDs'%subject_col)\n",
    "        else:\n",
    "            if subject_col not in df.columns:\n",
    "                raise IOError('could not find %s in any of the columns of your dataframe: %s'%(subject_col,\n",
    "                                                                                   df.columns))\n",
    "            else:\n",
    "                subject_IDs = df[subject_col].values\n",
    "        df.drop(subject_col, axis=1, inplace=True)\n",
    "    else:\n",
    "        subject_IDs = []\n",
    "    if len(cols_to_use) > 0:\n",
    "        df = df[cols_to_use]\n",
    "    if df.shape[0] != len(raw_paths):\n",
    "        raise IOError('number of scans (n=) does not match number of rows in spreadsheet (n=)'%(len(raw_paths),\n",
    "                                                                                               df.shape[0]))\n",
    "    \n",
    "    print('loading beta images')\n",
    "    if intercept == 'first':\n",
    "        int_img = ni.load(beta_paths[0]).get_data()\n",
    "        beta_paths.remove(beta_paths[0])\n",
    "    elif intercept == 'last':\n",
    "        int_img = ni.load(beta_paths[-1]).get_data()\n",
    "        beta_paths.remove(beta_paths[-1])\n",
    "    else:\n",
    "        int_img = None\n",
    "    jnk = ni.concat_images(beta_paths)\n",
    "    beta_imgs = jnk.get_data()\n",
    "    aff = jnk.affine\n",
    "    if len(beta_imgs.shape) > 4:\n",
    "        try:\n",
    "            x,y,z = beta_imgs.shape[:3]\n",
    "            s = beta_imgs.shape[-1]\n",
    "            beta_imgs.reshape(x,y,z,s)\n",
    "        except:\n",
    "            raise IOError('shape of beta images is %s, expecting a set of 3D images (so 4d)'%beta_imags.shape)\n",
    "    \n",
    "    if ni.load(beta_imgs[:,:,:,0]).shape != ni.load(raw_paths[0]).shape:\n",
    "        raise IOError('inconsistent dimensions between betas and raw images')\n",
    "    \n",
    "    if sdres_img:\n",
    "        if os.path.isfile(sdres_img):\n",
    "            sdres_img = ni.load(sdres_img).get_data()\n",
    "        else:\n",
    "            raise IOError('could not find any SD of residual images at path %s'%sdres)\n",
    "    else:\n",
    "        sdres_img = create_sdres_img(res_paths, res_str, beta_dir, memory_load)\n",
    "    \n",
    "    if type(image_mask) != type(None):\n",
    "        try:\n",
    "            mask = ni.load(image_mask).get_data()\n",
    "        except:\n",
    "            raise IOError('could not load mask. Please ensure the path points to an existing nifti image')\n",
    "        if mask.shape != ni.load(raw_paths[0]).shape:\n",
    "            raise IOError('dimensions of mask do not match the dimensions of input images')\n",
    "    else:\n",
    "        mask = None\n",
    "    \n",
    "    w_transform(beta_imgs, int_img, raw_paths, res_paths, sdres_img, \n",
    "                df, subject_IDs, aff, mask, outdir)\n",
    "    \n",
    "    print('FINISHED! W-SCORE images written to %s'%outdir)\n",
    "    \n",
    "def create_sdres_img(res_paths, res_str, beta_dir, memory_load):\n",
    "    \n",
    "    res_paths = glob(os.path.join(beta_dir,'%s*'%res_str))\n",
    "    print('calculating standard deviation of the residuals')\n",
    "    if memory_load == 'high':\n",
    "        print('loading res images...')\n",
    "        res_imgs = ni.concat_images(res_paths).get_data()\n",
    "        print('calculating...')\n",
    "        sdres_img = res_imgs.std(ddof=1,axis=3)\n",
    "    else:\n",
    "        sdres_img = ni.load(res_paths[0]).get_data()\n",
    "        for img in res_paths[1:]:\n",
    "            jnk = ni.load(img).get_data()\n",
    "            sdres_img += jnk\n",
    "        sdres_img = np.divide(sdres_img, len(res_paths))\n",
    "    \n",
    "    return sdres_img\n",
    "    \n",
    "def w_transform(beta_imgs, int_img, raw_paths, sdres_img, \n",
    "                df, subject_IDs, aff, mask, outdir):\n",
    "        \n",
    "    print('performing w-score transformations...')\n",
    "    df.index = range(len(df.shape[0]))\n",
    "    x,y,z = ni.load(beta_imgs[0]).shape\n",
    "    for i,scan in enumerate(raw_paths):\n",
    "        if len(subject_IDs) > 0:\n",
    "            sid = subject_IDs[i]\n",
    "        else:\n",
    "            sid = os.path.split(scan).split('.')[0]\n",
    "        coefs = []\n",
    "        if type(int_img) != type(None):\n",
    "            coefs.append(int_img)\n",
    "        for j,col in enumerate(df.columns):\n",
    "            val = df.loc[i,col]\n",
    "            coef = np.multiply(beta_imgs[:,:,:,j],val)\n",
    "            coefs.append(coef.reshape(x,y,z,1))\n",
    "        jnk = np.concatenate(coefs,axis=3)\n",
    "        predicted = jnk.sum(axis=3)\n",
    "        observed = ni.load(scan).get_data()\n",
    "        wscr_mat = (observed - predicted) / sdres_img\n",
    "        if type(mask) != type(None):\n",
    "            wscr_mat[msk==0] = 0\n",
    "        wscr_img = ni.Nifti1Image(wscr_mat,aff)\n",
    "        flnm = os.path.join(outdir,'WSCORE_%s'%(sid)\n",
    "        wscr_img.to_filename(flnm))\n",
    "        print('finished %s, %s of %s'%(sid,i+1,len(scans)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.concatenate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jnk = ni.load('/home/users/jvogel/Science/templates/templates/MNI152_T1_1mm.nii').get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [   3.,    3.,    5., ...,    0.,    0.,    0.],\n",
       "        [   2.,    2.,    3., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [ 189.,  189.,  194., ...,    0.,    0.,    0.],\n",
       "        [ 135.,  135.,  131., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [ 533.,  533.,  538., ...,    0.,    0.,    0.],\n",
       "        [ 372.,  372.,  371., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       ..., \n",
       "       [[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [  80.,   80.,   80., ...,    0.,    0.,    0.],\n",
       "        [  49.,   49.,   45., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]],\n",
       "\n",
       "       [[   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        ..., \n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.],\n",
       "        [   0.,    0.,    0., ...,    0.,    0.,    0.]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(jnk,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
